{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this week's exercise, we will apply neural networks to a new type of data - text. You will also pick your own corpus of text to play with.  **But first, some definitions** (there's a lot of new jargon in 'NLP'):\n",
    "* *Corpus*: The set of all text documents you want to work on\n",
    "* *Document*: An individual unit of text in your corpus\n",
    "\n",
    "Below are some **examples of corpora** to get you thinking:\n",
    "\n",
    "* Corpus of 100,000 IMDB reviews, where each document is an individual review\n",
    "* Corpus of 20 English novels, where each document is an individual novel\n",
    "* Corpus of one Malay novel, where each document is a chapter\n",
    "\n",
    "You can **browse for a corpus in the below links** (if you don't already have one in mind):\n",
    "\n",
    "* https://github.com/niderhoff/nlp-datasets\n",
    "* https://www.gutenberg.org/catalog/\n",
    "\n",
    "If this is your first time working with text, it's probably easier to deal with a corpus of many short documents - for example the IMDB review dataset, which is linked below in Chapter 2.  Play around with several corpora over the course of week, and work with something that interests you. Remember text doesn't have to be English (try other languages), or even a natural language (try code or musical notation)!\n",
    "\n",
    "**Key learning resources** for the week:\n",
    "* https://web.stanford.edu/~jurafsky/slp3/ - legendary textbook introducing key theory and concepts of working with text, up to deep learning methods\n",
    "* http://web.stanford.edu/class/cs224n/ - great course that introduces theory and concepts of text processing in the context of deep learning (can read class notes / assignments and skip videos if you are short on time) \n",
    "* https://course.fast.ai/index.html - fast.ai's introduction to deep learning (you'll have to pick out the bits about text and RNNs) is an efficient and effective way of tackling the topic\n",
    "* https://www.datacamp.com/courses/natural-language-processing-fundamentals-in-python - very hands on datacamp course that will let you practice using existing tools for NLP tasks\n",
    "\n",
    "Some **additional tools below** that can help in NLP (if you haven't found them already):\n",
    "* scikit-learn has a handy set of features for NLP\n",
    "* https://spacy.io/ - commercially oriented python package for NLP\n",
    "* https://www.nltk.org/ - slightly more academic oriented python package for NLP \n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('Reviews.csv')\n",
    "corpus = data['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre- processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import nltk\n",
    "# import string\n",
    "# import spacy\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "# from contractions import CONTRACTION_MAP\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# #function for tokenize text\n",
    "# def tokenize_text(text): \n",
    "#     tokens = nltk.word_tokenize(text) \n",
    "#     tokens = [token.strip() for token in tokens] #remove whitespace in tokens\n",
    "#     return tokens\n",
    "\n",
    "# #function for expand contractions\n",
    "# def expand_contractions(text, contraction_mapping): # contraction mapping is CONTRACTION_MAP from custom contraction.py file\n",
    "    \n",
    "#     contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), #from CONTRACTION_MAP in nltk\n",
    "#                                       flags=re.IGNORECASE|re.DOTALL)             # match all contraction keys in CONTRACTION_MAP\n",
    "#     def expand_match(contraction):\n",
    "#         match = contraction.group(0)\n",
    "#         first_char = match[0]\n",
    "#         expanded_contraction = contraction_mapping.get(match)\\\n",
    "#                                 if contraction_mapping.get(match)\\\n",
    "#                                 else contraction_mapping.get(match.lower()) #get key from dictionary + try lower case \n",
    "#         expanded_contraction = first_char+expanded_contraction[1:] #keep first char constant and add expanded contraction \n",
    "#                                                                    #from 2nd char onwards\n",
    "#         return expanded_contraction\n",
    "        \n",
    "#     expanded_text = contractions_pattern.sub(expand_match, text) # replace matched contraaction pattern with expanded one\n",
    "#     expanded_text = re.sub(\"'\", \"\", expanded_text) #remove the ' from expanded text\n",
    "#     return expanded_text\n",
    "\n",
    "# # function for POS tags \n",
    "# from nltk.corpus import wordnet as wn\n",
    "# def pos_tag_text(text): # convert spacy tags to wordnet tags to use wordnet lemmatizer\n",
    "#     def wn_tags(token_pos):\n",
    "#         if token_pos == 'ADJ':\n",
    "#             return wn.ADJ\n",
    "#         if token_pos == 'VERB':\n",
    "#             return wn.VERB\n",
    "#         if token_pos == 'NOUN':\n",
    "#             return wn.NOUN\n",
    "#         if token_pos == 'ADV':\n",
    "#             return wn.ADV\n",
    "#         else:\n",
    "#             return None\n",
    "#     text = nlp(text)\n",
    "#     tagged_text = [(token.orth_,token.pos_) for token in text]\n",
    "#     tagged_text = [(token[0].lower(),wn_tags(token[1])) for token in tagged_text] # convert tags and words to lowercase\n",
    "#     return tagged_text\n",
    "\n",
    "# # function to lemmatize text based on POS tags \n",
    "# wnl = WordNetLemmatizer()\n",
    "   \n",
    "# def lemmatize_text(text):\n",
    "    \n",
    "#     pos_tagged_text = pos_tag_text(text)\n",
    "#     lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag # return lemmatized word if pos tag present\n",
    "#                          else word                               # just return word if pos tag is \"None\"\n",
    "#                          for word, pos_tag in pos_tagged_text]\n",
    "#     lemmatized_text = ' '.join(lemmatized_tokens) #join tokens with \" \" between them\n",
    "#     return lemmatized_text\n",
    "\n",
    "# # function to remove special character\n",
    "# def remove_special_characters(text):\n",
    "#     tokens = tokenize_text(text)\n",
    "#     pattern = re.compile('[{}]'.format(re.escape(string.punctuation))) #punctuation string from re module\n",
    "#     filtered_tokens = [pattern.sub(' ', token) for token in tokens] #replace matching special character with \" \" \n",
    "#     filtered_text = ' '.join(filtered_tokens) #join tokens with \" \" between them\n",
    "#     return filtered_text\n",
    "\n",
    "# # function to remove stopwords\n",
    "# stopword_list = nltk.corpus.stopwords.words('english') #stopword list from nltk\n",
    "# stopword_list = stopword_list +['br','amazon','com','www','http']\n",
    "# def remove_stopwords(text):\n",
    "#     tokens = tokenize_text(text)\n",
    "#     filtered_tokens = [token for token in tokens if token not in stopword_list] # only extract tokens not in stopword list\n",
    "#     filtered_text = ' '.join(filtered_tokens) #join tokens with \" \" between them\n",
    "#     return filtered_text\n",
    "\n",
    "# # function to normalize corpus (list of sentences!!!!)\n",
    "# def normalize_corpus(corpus, lemmatize=True, tokenize=False):\n",
    "#     normalized_corpus = []    \n",
    "#     for text in corpus:\n",
    "#         text = expand_contractions(text, CONTRACTION_MAP)\n",
    "#         if lemmatize:\n",
    "#             text = lemmatize_text(text)\n",
    "#         else:\n",
    "#             text = text.lower()\n",
    "#         text = remove_special_characters(text)\n",
    "#         text = remove_stopwords(text)\n",
    "#         if tokenize:\n",
    "#             text = tokenize_text(text)\n",
    "#             normalized_corpus.append(text)\n",
    "#         else:\n",
    "#             normalized_corpus.append(text)\n",
    "            \n",
    "#     return normalized_corpus\n",
    "\n",
    "# prep_corpus = normalize_corpus(corpus[:100000]) #sample 1st 100,000 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# corpus_df = pd.DataFrame(prep_corpus,columns = ['Text'])\n",
    "# corpus_df.to_csv('prep2.csv',index=False,header = True)\n",
    "corpus_df = pd.read_csv('prep2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buy several vitality dog food product find good quality product look like stew process meat smell good labrador finicky appreciate product good'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df.loc[0,'Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#003366\"><b> [Amazon Fine Food Reviews] This dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories. Due to computational constraints, only the first 100,000 reviews were used for analysis\n",
    "    \n",
    "<span style=\"color:#003366\"><b> Preprocessing: (1) Expand contractions: it's to it is by looking for the ' character. (2) Part of Speech Tagging for more accurate lemmatization using Wordnet Lemmatizer. Lemmatization reducee words to the root word. 'Jumps','Jumping', \"Jumped' to 'Jump'. (3) Removed stopwords from nltk stopword list (common English characters like is, am, the). Customized the stopword list to include stopwords specific to Amazon reviews (amazon, www etc). (4) Special characters and punctuations are also removed and all characters are converted to lowercase "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: How do we turn text into data we can use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert your corpus into bags of words\n",
    "\n",
    "We can't apply any of the techniques we have learned over the past few weeks directly on raw text.  Therefore, our first task is to convert our corpus into numbers.  The simplest way to do this is to use a **bag of words**. You can see some examples of this here: https://liferay.de.dariah.eu/tatom/index.html.\n",
    "\n",
    "Once you understand the concept, convert your corpus and documents into bags of words below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 10000, ngram_range=(1, 2))\n",
    "tf_dtm = vectorizer.fit_transform(corpus_df['Text'])\n",
    "print(tf_dtm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show us your bags\n",
    "\n",
    "Show and explain what one of your documents looks like as a bag of words below.  What are the advantages and disadvantages of encoding text as bags of words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>00 per</th>\n",
       "      <th>000</th>\n",
       "      <th>0g</th>\n",
       "      <th>10</th>\n",
       "      <th>10 00</th>\n",
       "      <th>10 12</th>\n",
       "      <th>10 15</th>\n",
       "      <th>10 day</th>\n",
       "      <th>10 gram</th>\n",
       "      <th>...</th>\n",
       "      <th>zip</th>\n",
       "      <th>zip lock</th>\n",
       "      <th>zipfizz</th>\n",
       "      <th>ziploc</th>\n",
       "      <th>ziploc bag</th>\n",
       "      <th>ziplock</th>\n",
       "      <th>ziplock bag</th>\n",
       "      <th>ziwipeak</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zukes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  00 per  000  0g  10  10 00  10 12  10 15  10 day  10 gram  ...    zip  \\\n",
       "0   0       0    0   0   0      0      0      0       0        0  ...      0   \n",
       "1   0       0    0   0   0      0      0      0       0        0  ...      0   \n",
       "2   0       0    0   0   0      0      0      0       0        0  ...      0   \n",
       "3   0       0    0   0   0      0      0      0       0        0  ...      0   \n",
       "4   0       0    0   0   0      0      0      0       0        0  ...      0   \n",
       "\n",
       "   zip lock  zipfizz  ziploc  ziploc bag  ziplock  ziplock bag  ziwipeak  zoe  \\\n",
       "0         0        0       0           0        0            0         0    0   \n",
       "1         0        0       0           0        0            0         0    0   \n",
       "2         0        0       0           0        0            0         0    0   \n",
       "3         0        0       0           0        0            0         0    0   \n",
       "4         0        0       0           0        0            0         0    0   \n",
       "\n",
       "   zukes  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 10000 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tf_dtm.toarray(),columns = vectorizer.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#003366\"><b> Advantages: It is easy to transform the unstructured text to numerical features in a structured table of numerical values with each unique word as a feature and the count of the word as the value in a table form. This structured format makes it convenient to use existing machine learning methods such as random forest, naive bayes and SVM to train models. It is also easily interpretable by the user.\n",
    "    \n",
    "<span style=\"color:#003366\"><b> Disadvantages: Using term frequencies, a word that appear frequently in the document is considered to be more important in characterizing the document. This is a naive representation as there are some words in English that tends to appear more frequently than others. In fact, words that appear very frequently in most documents are likely to be not useful in differentiating the documents. Also, since it is a bag of words approach, the order of the words are not considered. This is where RNN networks would become more useful. Although the use of bigrams of trigrams can group words that co-occured togehter. Long range ordering information in the documents are lost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tell us a story with your bags\n",
    "Now that your text is in a more digestible format, you can apply previously learned techniques to better understand the corpus. **Create a brief story around your corpus, for example by using clustering techniques.** Some examples of what you can do below:\n",
    "* Use *Hierarchical Clustering* to understand similarity of documents in your corpus. What distance measure works best? Are the results what you expect?\n",
    "* Learn about *Latent Dirichlet Allocation* to extract topics from your corpora, and measure each document on how much of each topic it contains. How do you interpret these topics?\n",
    "\n",
    "Some **potential inspiration** below (but please keep your own story simple!):\n",
    "* https://liferay.de.dariah.eu/tatom/topic_model_mallet.html covers a few examples of text analysis\n",
    "* http://fantheory.viacom.com/\n",
    "* https://pudding.cool/2017/02/vocabulary/\n",
    "\n",
    "Additional resources on LDA (if you are interested): \n",
    "* https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d\n",
    "* https://www.youtube.com/watch?v=DDq3OVp9dNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10000)\n"
     ]
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "corpus_df2 = corpus_df.sample(10000,axis=0,random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer2 = CountVectorizer(max_features = 10000, ngram_range=(1, 2))\n",
    "tf_dtm2 = vectorizer2.fit_transform(corpus_df2['Text'])\n",
    "print(tf_dtm2.shape)\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "Z1 = linkage(tf_dtm2.toarray(),method='complete',metric='correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "f,ax = plt.subplots(figsize=(20, 12))\n",
    "dendrogram(Z1,truncate_mode='level',p=3,leaf_font_size=10)\n",
    "plt.ylim((1.004,1.01))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4742</td>\n",
       "      <td>tea</td>\n",
       "      <td>good</td>\n",
       "      <td>taste</td>\n",
       "      <td>coffee</td>\n",
       "      <td>like</td>\n",
       "      <td>product</td>\n",
       "      <td>one</td>\n",
       "      <td>great</td>\n",
       "      <td>flavor</td>\n",
       "      <td>...</td>\n",
       "      <td>buy</td>\n",
       "      <td>use</td>\n",
       "      <td>get</td>\n",
       "      <td>make</td>\n",
       "      <td>try</td>\n",
       "      <td>would</td>\n",
       "      <td>find</td>\n",
       "      <td>drink</td>\n",
       "      <td>order</td>\n",
       "      <td>bag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1351</td>\n",
       "      <td>food</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>like</td>\n",
       "      <td>eat</td>\n",
       "      <td>treat</td>\n",
       "      <td>love</td>\n",
       "      <td>get</td>\n",
       "      <td>good</td>\n",
       "      <td>...</td>\n",
       "      <td>make</td>\n",
       "      <td>product</td>\n",
       "      <td>one</td>\n",
       "      <td>try</td>\n",
       "      <td>give</td>\n",
       "      <td>use</td>\n",
       "      <td>buy</td>\n",
       "      <td>find</td>\n",
       "      <td>taste</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3907</td>\n",
       "      <td>like</td>\n",
       "      <td>taste</td>\n",
       "      <td>flavor</td>\n",
       "      <td>good</td>\n",
       "      <td>make</td>\n",
       "      <td>try</td>\n",
       "      <td>get</td>\n",
       "      <td>one</td>\n",
       "      <td>would</td>\n",
       "      <td>...</td>\n",
       "      <td>product</td>\n",
       "      <td>great</td>\n",
       "      <td>use</td>\n",
       "      <td>chocolate</td>\n",
       "      <td>eat</td>\n",
       "      <td>find</td>\n",
       "      <td>buy</td>\n",
       "      <td>really</td>\n",
       "      <td>chip</td>\n",
       "      <td>much</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      n     0      1       2       3     4        5     6      7       8  \\\n",
       "0  4742   tea   good   taste  coffee  like  product   one  great  flavor   \n",
       "1  1351  food    dog     cat    like   eat    treat  love    get    good   \n",
       "2  3907  like  taste  flavor    good  make      try   get    one   would   \n",
       "\n",
       "   ...         10       11   12         13    14     15    16      17     18  \\\n",
       "0  ...        buy      use  get       make   try  would  find   drink  order   \n",
       "1  ...       make  product  one        try  give    use   buy    find  taste   \n",
       "2  ...    product    great  use  chocolate   eat   find   buy  really   chip   \n",
       "\n",
       "      19  \n",
       "0    bag  \n",
       "1  great  \n",
       "2   much  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_df = pd.DataFrame(tf_dtm2.toarray(), columns = vectorizer2.get_feature_names())\n",
    "tf_df['Cluster'] = fcluster(Z1, 3, criterion='maxclust')\n",
    "\n",
    "n_topic1 = tf_df.loc[tf_df['Cluster']==1,:].shape[0]\n",
    "n_topic2 = tf_df.loc[tf_df['Cluster']==2,:].shape[0]\n",
    "n_topic3 = tf_df.loc[tf_df['Cluster']==3,:].shape[0]\n",
    "n_topics = [n_topic1,n_topic2,n_topic3]\n",
    "number = pd.DataFrame(n_topics)\n",
    "number.columns = ['n']\n",
    "\n",
    "tf_df_sum = tf_df.groupby('Cluster').sum()\n",
    "top20_topic1 = pd.DataFrame(tf_df_sum.iloc[0,:-1].sort_values(ascending=False)[:20].index).T\n",
    "top20_topic2 = pd.DataFrame(tf_df_sum.iloc[1,:-1].sort_values(ascending=False)[:20].index).T\n",
    "top20_topic3 = pd.DataFrame(tf_df_sum.iloc[2,:-1].sort_values(ascending=False)[:20].index).T\n",
    "\n",
    "top20 = pd.concat([top20_topic1,top20_topic2,top20_topic3],axis=0).reset_index(drop=True)\n",
    "\n",
    "summary = pd.concat([number,top20],axis=1)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#003366\"><b> 10,000 reviews were sampled due to memory resource constraints. Correlation metric(mean-centered cosine distance) was used as the distance metric to compare the similarity of the reivews. Complete linkage was used as it gave the most balanced dendrogram. The dendrogram was cut to yield 3 clusters or topics and the top 20 words with the highest tf score is shown in the table above. The topics are numbered according to the position of the branch on the dendrogram from left to right.\n",
    "    \n",
    "<span style=\"color:#003366\"><b>Topic 1: Tea and cofee <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 2: Pet food <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 3: Snacks like chocolate and chip(s) \n",
    "    \n",
    "<span style=\"color:#003366\"><b> Unlike LDA and NMF performed below, each document can only have 1 topic. However, the hierarachical relationship of the topics can inferred from the dendrogram. For example,topic 2 and topic 3 can merge together to form a food topic. In LDA and NMF, the topic were assumed to be independent of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# parameters = {'n_components':[3,4,5,7,10]} \n",
    "# lda0 = LatentDirichletAllocation(random_state =42)\n",
    "# lda = GridSearchCV(lda0, parameters, cv= 3,n_jobs=-1) # default scoring of LDA, max likelihood\n",
    "# lda.fit(tf_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator LatentDirichletAllocation from version 0.19.2 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator GridSearchCV from version 0.19.2 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# pickle.dump(lda, open('lda.pickle', 'wb'))\n",
    "lda = pickle.load(open('lda.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'n_components': 3}, -11985953.748180076),\n",
       " ({'n_components': 4}, -11992511.308440186),\n",
       " ({'n_components': 5}, -12037835.763388896),\n",
       " ({'n_components': 7}, -12066751.079387205),\n",
       " ({'n_components': 10}, -12123028.299006099)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(lda.cv_results_['params'],lda.cv_results_['mean_test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tea</td>\n",
       "      <td>coffee</td>\n",
       "      <td>taste</td>\n",
       "      <td>flavor</td>\n",
       "      <td>like</td>\n",
       "      <td>drink</td>\n",
       "      <td>cup</td>\n",
       "      <td>good</td>\n",
       "      <td>try</td>\n",
       "      <td>one</td>\n",
       "      <td>make</td>\n",
       "      <td>use</td>\n",
       "      <td>water</td>\n",
       "      <td>would</td>\n",
       "      <td>great</td>\n",
       "      <td>get</td>\n",
       "      <td>product</td>\n",
       "      <td>strong</td>\n",
       "      <td>sugar</td>\n",
       "      <td>find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>food</td>\n",
       "      <td>dog</td>\n",
       "      <td>get</td>\n",
       "      <td>product</td>\n",
       "      <td>love</td>\n",
       "      <td>one</td>\n",
       "      <td>buy</td>\n",
       "      <td>would</td>\n",
       "      <td>like</td>\n",
       "      <td>treat</td>\n",
       "      <td>good</td>\n",
       "      <td>price</td>\n",
       "      <td>eat</td>\n",
       "      <td>find</td>\n",
       "      <td>order</td>\n",
       "      <td>time</td>\n",
       "      <td>use</td>\n",
       "      <td>go</td>\n",
       "      <td>store</td>\n",
       "      <td>give</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>taste</td>\n",
       "      <td>like</td>\n",
       "      <td>good</td>\n",
       "      <td>flavor</td>\n",
       "      <td>make</td>\n",
       "      <td>great</td>\n",
       "      <td>chocolate</td>\n",
       "      <td>eat</td>\n",
       "      <td>try</td>\n",
       "      <td>love</td>\n",
       "      <td>use</td>\n",
       "      <td>one</td>\n",
       "      <td>would</td>\n",
       "      <td>bar</td>\n",
       "      <td>product</td>\n",
       "      <td>snack</td>\n",
       "      <td>chip</td>\n",
       "      <td>add</td>\n",
       "      <td>really</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0       1      2        3     4      5          6      7     8      9   \\\n",
       "0    tea  coffee  taste   flavor  like  drink        cup   good   try    one   \n",
       "1   food     dog    get  product  love    one        buy  would  like  treat   \n",
       "2  taste    like   good   flavor  make  great  chocolate    eat   try   love   \n",
       "\n",
       "     10     11     12     13       14     15       16      17      18    19  \n",
       "0  make    use  water  would    great    get  product  strong   sugar  find  \n",
       "1  good  price    eat   find    order   time      use      go   store  give  \n",
       "2   use    one  would    bar  product  snack     chip     add  really   buy  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer=vectorizer, lda_model=lda, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=lda.best_estimator_, n_words=20)        \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#003366\"><b> LDA gave the best results (maximum cross-validated log likelihood) with 3 topics. Log likelihood is a convenient metric to use as it is inbuilt into sklearn but the topics may not be human interpretable\n",
    "    \n",
    "<span style=\"color:#003366\"><b>Topic 1: Tea and cofee <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 2: Pet food <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 3: Snacks like chocolate and chip(s)\n",
    "    \n",
    "<span style=\"color:#003366\"><b>From the visualization below, the topic quality are good as they form big, well-seperated circles. Location of the circles represents position of the topic in the keyword space projected into 2D for visualizaiton. If topics contain very different keywords, they will be far apart from each other. The size of the circles indicates the overall prevlance of the topic in the corpus. A large circle means that the topic has a high occurence probability in many documents in the corpus. \n",
    "    \n",
    "http://www.aclweb.org/anthology/W14-3110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el23214116091239606037839869\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el23214116091239606037839869_data = {\"mdsDat\": {\"x\": [-632.0454711914062, -1209.6654052734375, -1437.4532470703125], \"y\": [-384.3250732421875, 212.16412353515625, -586.322509765625], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [39.01108109278953, 34.51168488680238, 26.47723402040809]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"Freq\": [27293.0, 25392.0, 43781.0, 16172.0, 22656.0, 14295.0, 36988.0, 26491.0, 12342.0, 13011.0, 8845.0, 8364.0, 8313.0, 8855.0, 22764.0, 6826.0, 5945.0, 10432.0, 6678.0, 11353.0, 5481.0, 4345.0, 4034.0, 5223.0, 52118.0, 4404.0, 12651.0, 10249.0, 5372.0, 15166.0, 22656.00619175223, 8854.758842804113, 4157.110356769435, 3375.2315983122394, 3892.6835320490322, 2480.9407535477435, 2258.2476682686706, 2342.692382314583, 2063.1275150726465, 2113.3129825278543, 1915.4370618355827, 2203.195330100367, 1913.301205310575, 1501.4901856909778, 1498.6337032257813, 1829.7298665003975, 1502.9648500779033, 1330.9236851009612, 1492.9557442100386, 1324.63856875603, 1124.2673934530344, 1207.6950316510538, 1226.57280511361, 995.8046251747967, 941.9439188244473, 12333.736050618048, 901.9372600853561, 997.4355515797573, 926.6707317773139, 851.656813554486, 3638.5795480389725, 2298.2156249878494, 3714.617635438547, 1149.3378428199792, 1050.4988918330623, 2597.5888377296787, 5174.70931471731, 24303.869220960747, 6162.376842020673, 1735.1653010768762, 9655.749514666088, 2004.690130010186, 4105.114285507899, 8044.273631182813, 11118.685806004267, 10991.160570639182, 20699.693979047497, 4246.601585419446, 20122.45329017995, 4285.860390836086, 10799.52780144103, 14756.918021809632, 7803.542305479311, 16595.319373832976, 9397.16581407579, 3926.139606167439, 15007.38037109928, 13375.765891987223, 9750.092579409005, 10997.996544870653, 10997.135771626776, 5822.160395296936, 7891.452384329904, 6595.397917891821, 10574.138541889664, 5494.778003159539, 11900.080839438557, 12427.689832867109, 9320.215887343713, 9125.552260511582, 7986.913116833557, 6599.312584818872, 6629.513082927485, 8844.736300972943, 8312.572710816812, 8363.837353733978, 6826.240425208089, 5480.439960984987, 4030.419082748412, 3952.564091045835, 3869.106568534296, 3673.64330915416, 3555.341002263436, 4403.509453491374, 3180.2087527520753, 3173.340613814532, 3039.4487124121724, 3099.373057603807, 3020.0446673626643, 2601.9963498699876, 2665.018798584221, 2266.9409696224307, 2298.590431187068, 2193.3582595268385, 2163.746017031646, 2213.1229451448676, 2015.7724320762552, 1875.1850401861657, 1824.5105169414946, 1728.836774139972, 1578.6292687761484, 1540.7237271879144, 1653.9562954015632, 3948.7505599900746, 3739.233789749828, 2025.0960828364705, 5075.226711957099, 11921.407102132813, 5070.202834446972, 3783.715489154176, 24447.550495270334, 4982.958940324906, 4257.463040743526, 18808.23442959018, 5487.9793723535695, 20749.80764927568, 22592.17014740972, 8110.050683384362, 11766.516643886269, 13589.447652794532, 13018.475699906767, 6393.769944591262, 11594.017460043538, 9997.149847208455, 10194.679152488852, 7611.702842976342, 9387.160071214008, 9891.009345212047, 5462.77126543115, 8633.880152019658, 5653.411235258186, 7168.281801066326, 5941.53177345132, 6364.021513722643, 5845.542139123046, 5939.874802023192, 6215.33136156279, 5609.310555866887, 5584.417721984278, 27292.607491661754, 25392.148029246684, 16172.309621736678, 14292.906836770711, 4344.706684093781, 3077.8770720590237, 2991.7240240950696, 3081.2291423999472, 2924.872384709439, 2502.2943435939073, 4033.1666970082747, 2049.0167220777694, 2635.48422189644, 1733.318468042702, 1708.0631974927526, 1624.410995104468, 1561.8481858638968, 1545.4115365461034, 1506.6923265438356, 1493.8745487658925, 1592.2874653135768, 1524.8567172125724, 1567.7760385483746, 1322.407471567502, 1311.910734379075, 1258.8292147034556, 1215.2342990586878, 2975.8899273112875, 2975.8357891150604, 1240.6671469375326, 3002.220423970439, 3002.9483697726905, 1778.8228215141855, 5450.476394906435, 5799.610349352032, 2698.2896227362003, 7502.5219485959715, 2937.9244200889875, 17871.67227184743, 19297.691713267785, 17098.35260095241, 2317.0434339317967, 5699.486767917964, 2666.752510855375, 10082.117801272265, 12797.367425118844, 8904.30915099287, 8068.60047128981, 8200.867707420732, 7063.649382077302, 6815.596153071124, 5557.199739144795, 5121.151279729503, 5983.686560056139, 5881.4112129056775, 5470.822568059598, 4861.291918449347, 4374.39395735628, 4009.1544691548, 3940.4719388157346, 3973.216500613067], \"Term\": [\"tea\", \"coffee\", \"taste\", \"drink\", \"dog\", \"cup\", \"flavor\", \"food\", \"treat\", \"chocolate\", \"bar\", \"snack\", \"chip\", \"cat\", \"eat\", \"salt\", \"green\", \"water\", \"strong\", \"sugar\", \"sauce\", \"blend\", \"juice\", \"fat\", \"like\", \"texture\", \"add\", \"sweet\", \"oil\", \"price\", \"dog\", \"cat\", \"shipping\", \"dog food\", \"week\", \"dog love\", \"pet\", \"teeth\", \"feed\", \"hair\", \"vet\", \"year old\", \"fee\", \"cat food\", \"toy\", \"lb\", \"formula\", \"puppy\", \"service\", \"shampoo\", \"greenies\", \"lick\", \"seller\", \"training\", \"kibble\", \"treat\", \"month old\", \"condition\", \"door\", \"science\", \"arrive\", \"ship\", \"chew\", \"shipment\", \"local store\", \"baby\", \"month\", \"food\", \"old\", \"online\", \"store\", \"send\", \"local\", \"year\", \"price\", \"order\", \"get\", \"problem\", \"product\", \"last\", \"time\", \"buy\", \"day\", \"love\", \"give\", \"happy\", \"one\", \"would\", \"go\", \"eat\", \"find\", \"purchase\", \"bag\", \"box\", \"use\", \"two\", \"good\", \"like\", \"great\", \"make\", \"try\", \"well\", \"much\", \"bar\", \"chip\", \"snack\", \"salt\", \"sauce\", \"cheese\", \"butter\", \"cooky\", \"gluten\", \"peanut\", \"texture\", \"pasta\", \"nut\", \"noodle\", \"almond\", \"popcorn\", \"gluten free\", \"pepper\", \"salty\", \"peanut butter\", \"bread\", \"cookie\", \"crunchy\", \"granola\", \"cracker\", \"sodium\", \"olive\", \"salad\", \"bake\", \"cake\", \"cook\", \"cereal\", \"fiber\", \"fat\", \"chocolate\", \"oil\", \"rice\", \"taste\", \"calorie\", \"tasty\", \"flavor\", \"delicious\", \"good\", \"like\", \"add\", \"eat\", \"make\", \"great\", \"sweet\", \"try\", \"use\", \"love\", \"really\", \"would\", \"one\", \"mix\", \"product\", \"sugar\", \"buy\", \"little\", \"find\", \"also\", \"much\", \"get\", \"best\", \"well\", \"tea\", \"coffee\", \"drink\", \"cup\", \"blend\", \"brew\", \"green tea\", \"roast\", \"bitter\", \"soda\", \"juice\", \"keurig\", \"smooth\", \"cup coffee\", \"starbucks\", \"caffeine\", \"espresso\", \"iced\", \"chai\", \"beverage\", \"sweetener\", \"leaf\", \"aroma\", \"pod\", \"refreshing\", \"tea bag\", \"decaf\", \"href\", \"href gp\", \"weak\", \"gp product\", \"gp\", \"french\", \"green\", \"strong\", \"ginger\", \"water\", \"black\", \"flavor\", \"taste\", \"like\", \"syrup\", \"sugar\", \"energy\", \"try\", \"good\", \"one\", \"use\", \"make\", \"would\", \"great\", \"find\", \"really\", \"get\", \"product\", \"love\", \"buy\", \"much\", \"best\", \"add\", \"well\"], \"Total\": [27293.0, 25392.0, 43781.0, 16172.0, 22656.0, 14295.0, 36988.0, 26491.0, 12342.0, 13011.0, 8845.0, 8364.0, 8313.0, 8855.0, 22764.0, 6826.0, 5945.0, 10432.0, 6678.0, 11353.0, 5481.0, 4345.0, 4034.0, 5223.0, 52118.0, 4404.0, 12651.0, 10249.0, 5372.0, 15166.0, 22656.684135518673, 8855.43574914287, 4157.870805313819, 3375.9078645603986, 3893.578046348509, 2481.6164178164304, 2258.925467380629, 2343.4615393401955, 2063.8250889419865, 2114.0412929352374, 1916.1099277259377, 2203.9717182016334, 1914.021100394395, 1502.1648771121304, 1499.3076954982193, 1830.5566710606736, 1503.6919049093522, 1331.6016678292929, 1493.7223414824866, 1325.3232527455655, 1124.9390364143744, 1208.4305247798293, 1227.327736836566, 996.4902333406652, 942.6211389881761, 12342.901473350865, 902.6315301311905, 998.2228536277678, 927.4162633010583, 852.3576331979466, 3641.885256323312, 2305.669079510316, 3759.3773845345604, 1151.0106133018626, 1051.4056482318024, 2631.310371705032, 5350.1045341851495, 26491.122528894786, 6722.701306856943, 1789.112943345816, 11749.213225573487, 2106.393639966124, 4728.850956429829, 10252.164252581617, 15166.84681660601, 15363.937026742238, 32898.71190066642, 5192.856161692147, 34637.744655105285, 5339.102887287506, 17536.62755383565, 26786.491741325306, 11740.378101735136, 32260.821094381423, 15482.372553829411, 4893.539311187385, 33802.6988673042, 29826.575345278536, 18857.881571308313, 22764.8703024566, 22918.357024494213, 9106.96348412532, 16213.486087065361, 11747.110225424589, 28639.888860387928, 8658.106860422402, 45447.25591383308, 52118.212581229236, 29154.287740321604, 30915.867620726844, 29663.04837814936, 16156.946807416218, 16943.781842306955, 8845.442740624747, 8313.266103158838, 8364.56885921129, 6826.94116120973, 5481.139192755337, 4031.1316314551323, 3953.2831836963114, 3869.839796744037, 3674.3571129362913, 3556.0655595240773, 4404.426721625264, 3180.8965583833924, 3174.059552106977, 3040.143501964448, 3100.0863038335688, 3020.7407870783645, 2602.6850969993598, 2665.7655363623717, 2267.633258737018, 2299.299071824982, 2194.058777176374, 2164.463140444072, 2213.8586760983385, 2016.463653333708, 1875.9026423209357, 1825.2675546793891, 1729.558709447527, 1579.3361365943026, 1541.4407989490348, 1654.7289921456238, 3967.4331191108618, 3766.318318104465, 2028.6837210800925, 5223.888382291158, 13011.681820184189, 5372.184709390087, 4369.594431439529, 43781.04443916555, 6200.214568159712, 5227.52902526629, 36988.76775779996, 7318.705199171923, 45447.25591383308, 52118.212581229236, 12651.346448011642, 22764.8703024566, 30915.867620726844, 29154.287740321604, 10249.762503384914, 29663.04837814936, 28639.888860387928, 32260.821094381423, 18425.086823777187, 29826.575345278536, 33802.6988673042, 9694.11077007843, 34637.744655105285, 11353.251449924166, 26786.491741325306, 14914.118353815062, 22918.357024494213, 15131.700431688829, 16943.781842306955, 32898.71190066642, 13079.156102469311, 16156.946807416218, 27293.27054440458, 25392.81225305969, 16172.99149261966, 14295.124667480324, 4345.575721593494, 3078.5346139032245, 2992.3814686966375, 3081.969875272097, 2925.586975324978, 2502.9820602057966, 4034.428310962176, 2049.6773002869145, 2636.449266806175, 1733.9778770605321, 1708.7322846293034, 1625.0729383512848, 1562.510848068987, 1546.0725374450146, 1507.3562626600344, 1494.552182693814, 1593.0501474903347, 1525.5968690237114, 1568.5661220671002, 1323.0769179755007, 1312.5972802726155, 1259.4888410994934, 1215.8924632618077, 2977.541695809891, 2977.5385826602374, 1241.4116282666841, 3004.225188571059, 3005.851202365557, 1789.124368949201, 5945.3400007095615, 6678.4572554927545, 3007.1279234892054, 10432.287825794356, 3528.5658849656934, 36988.76775779996, 43781.04443916555, 52118.212581229236, 2805.0082437081537, 11353.251449924166, 3485.9711339921637, 29663.04837814936, 45447.25591383308, 33802.6988673042, 28639.888860387928, 30915.867620726844, 29826.575345278536, 29154.287740321604, 22918.357024494213, 18425.086823777187, 32898.71190066642, 34637.744655105285, 32260.821094381423, 26786.491741325306, 16943.781842306955, 13079.156102469311, 12651.346448011642, 16156.946807416218], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9413, 0.9412, 0.9411, 0.9411, 0.9411, 0.9411, 0.941, 0.941, 0.941, 0.941, 0.941, 0.941, 0.9409, 0.9409, 0.9409, 0.9409, 0.9408, 0.9408, 0.9408, 0.9408, 0.9407, 0.9407, 0.9407, 0.9406, 0.9406, 0.9406, 0.9406, 0.9405, 0.9405, 0.9405, 0.9404, 0.9381, 0.9293, 0.9399, 0.9405, 0.9284, 0.908, 0.8552, 0.8543, 0.9107, 0.7451, 0.8918, 0.7999, 0.6988, 0.6308, 0.6064, 0.478, 0.7402, 0.3982, 0.7216, 0.4565, 0.3451, 0.5329, 0.2766, 0.442, 0.7211, 0.1293, 0.1394, 0.2817, 0.2138, 0.207, 0.494, 0.2213, 0.3641, -0.0551, 0.4866, -0.3987, -0.4923, -0.1991, -0.2789, -0.3708, 0.0459, 0.003, 1.0638, 1.0638, 1.0638, 1.0638, 1.0637, 1.0637, 1.0637, 1.0637, 1.0637, 1.0637, 1.0637, 1.0637, 1.0636, 1.0636, 1.0636, 1.0636, 1.0636, 1.0636, 1.0636, 1.0636, 1.0636, 1.0635, 1.0635, 1.0635, 1.0635, 1.0635, 1.0635, 1.0634, 1.0634, 1.0634, 1.0592, 1.0567, 1.0621, 1.035, 0.9764, 1.006, 0.9199, 0.4812, 0.8453, 0.8586, 0.3876, 0.776, 0.2799, 0.228, 0.6192, 0.4039, 0.2419, 0.2576, 0.5919, 0.1245, 0.0114, -0.0881, 0.1798, -0.0922, -0.165, 0.4903, -0.3254, 0.3666, -0.2544, 0.1435, -0.2174, 0.1128, 0.0157, -0.6025, 0.2173, 0.0015, 1.3289, 1.3289, 1.3288, 1.3287, 1.3287, 1.3287, 1.3287, 1.3286, 1.3286, 1.3286, 1.3286, 1.3286, 1.3285, 1.3285, 1.3285, 1.3285, 1.3285, 1.3285, 1.3284, 1.3284, 1.3284, 1.3284, 1.3284, 1.3284, 1.3284, 1.3284, 1.3283, 1.3283, 1.3283, 1.3283, 1.3282, 1.3279, 1.3231, 1.242, 1.1878, 1.2205, 0.9992, 1.1457, 0.6015, 0.5097, 0.2144, 1.1378, 0.6398, 1.061, 0.2497, 0.0616, -0.0051, 0.0621, 0.0019, -0.1116, -0.1245, -0.088, 0.0486, -0.3755, -0.4443, -0.4455, -0.3777, -0.0252, 0.1464, 0.1624, -0.0739], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.3895, -5.3289, -6.0851, -6.2934, -6.1508, -6.6012, -6.6953, -6.6586, -6.7857, -6.7616, -6.8599, -6.72, -6.8611, -7.1034, -7.1053, -6.9057, -7.1024, -7.224, -7.1091, -7.2287, -7.3928, -7.3212, -7.3057, -7.5141, -7.5697, -4.9975, -7.6131, -7.5125, -7.586, -7.6705, -6.2183, -6.6778, -6.1976, -7.3707, -7.4606, -6.5553, -5.8661, -4.3192, -5.6914, -6.9588, -5.2423, -6.8144, -6.0977, -5.4249, -5.1013, -5.1128, -4.4798, -6.0638, -4.508, -6.0546, -5.1304, -4.8182, -5.4553, -4.7008, -5.2695, -6.1422, -4.8013, -4.9164, -5.2326, -5.1122, -5.1123, -5.7482, -5.4441, -5.6235, -5.1515, -5.8061, -5.0333, -4.99, -5.2777, -5.2988, -5.4321, -5.6229, -5.6184, -5.2075, -5.2696, -5.2634, -5.4666, -5.6862, -5.9935, -6.013, -6.0343, -6.0862, -6.1189, -5.9049, -6.2304, -6.2326, -6.2757, -6.2561, -6.2821, -6.4311, -6.4071, -6.5689, -6.555, -6.6019, -6.6155, -6.5929, -6.6863, -6.7586, -6.786, -6.8399, -6.9308, -6.9551, -6.8842, -6.0139, -6.0685, -6.6817, -5.763, -4.909, -5.764, -6.0566, -4.1908, -5.7813, -5.9387, -4.453, -5.6848, -4.3548, -4.2697, -5.2942, -4.9221, -4.778, -4.821, -5.532, -4.9368, -5.085, -5.0655, -5.3577, -5.148, -5.0957, -5.6894, -5.2316, -5.6551, -5.4177, -5.6054, -5.5367, -5.6217, -5.6056, -5.5603, -5.6629, -5.6674, -3.8157, -3.8879, -4.339, -4.4626, -5.6534, -5.9981, -6.0265, -5.997, -6.0491, -6.2051, -5.7278, -6.405, -6.1533, -6.5723, -6.587, -6.6372, -6.6765, -6.687, -6.7124, -6.721, -6.6572, -6.7004, -6.6727, -6.8429, -6.8508, -6.8921, -6.9274, -6.0318, -6.0318, -6.9067, -6.023, -6.0227, -6.5464, -5.4266, -5.3645, -6.1297, -5.1071, -6.0446, -4.2391, -4.1623, -4.2833, -6.282, -5.3819, -6.1415, -4.8116, -4.5731, -4.9358, -5.0343, -5.0181, -5.1674, -5.2031, -5.4072, -5.4889, -5.3333, -5.3505, -5.4229, -5.541, -5.6466, -5.7337, -5.751, -5.7427]}, \"token.table\": {\"Topic\": [1, 2, 3, 2, 1, 2, 3, 3, 1, 2, 3, 1, 2, 1, 2, 3, 2, 2, 1, 2, 3, 3, 3, 1, 2, 3, 2, 3, 1, 2, 3, 2, 3, 2, 1, 2, 3, 3, 2, 2, 3, 1, 1, 1, 2, 3, 2, 1, 2, 2, 2, 3, 3, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 1, 2, 3, 3, 1, 2, 3, 1, 1, 1, 1, 3, 1, 2, 1, 2, 3, 3, 1, 2, 1, 1, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 2, 2, 1, 2, 3, 1, 2, 3, 1, 3, 1, 3, 2, 1, 2, 3, 1, 2, 3, 3, 1, 1, 1, 2, 3, 1, 3, 1, 3, 3, 2, 3, 3, 1, 1, 2, 3, 1, 3, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 1, 2, 3, 2, 2, 1, 2, 3, 1, 2, 3, 2, 1, 2, 3, 1, 3, 1, 2, 3, 2, 2, 2, 2, 1, 3, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3, 3, 1, 2, 3, 2, 2, 2, 2, 1, 1, 1, 2, 3, 1, 1, 1, 3, 1, 3, 1, 2, 3, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 3, 2, 3, 1, 2, 3, 1, 2, 3, 3, 3, 1, 1, 2, 1, 2, 3, 1, 1, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 3, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1], \"Freq\": [0.047504825076895796, 0.6410384881424708, 0.3114293025007811, 0.9996495891639451, 0.35376063808332736, 0.3863412460741886, 0.2599839996674406, 0.9996390830713887, 0.9992077574881574, 0.0008237491817709459, 0.0002745830605903153, 0.9873407667665416, 0.012541279947381013, 0.48669360541131285, 0.3192404133327785, 0.19403600083943612, 0.9997140344609178, 0.9999499470362616, 0.2646195192476197, 0.42885029860153095, 0.30651824694126184, 0.9996305363571724, 0.9997993649377275, 0.08133616017284324, 0.08615398150712315, 0.832632887065552, 0.00023011910597505516, 0.9998675154616147, 0.5614146690925111, 0.26236239729236077, 0.17621355042024234, 0.999517434451899, 0.9998263414350418, 0.9999283674649265, 0.5509120097736947, 0.26759756631143483, 0.18147206610489464, 0.9993397598803329, 0.9995594492215438, 0.8036818637841119, 0.19628352964584872, 0.9999507930320751, 0.9992245344503262, 0.007168804577725846, 0.9927466783747014, 0.999763650658534, 0.9997192769776353, 0.9881955494233908, 0.0117040657266835, 0.9999679905399953, 0.9161767221749701, 0.08377087720583151, 0.9999680124811858, 0.9987749693133916, 0.004536938483800822, 0.9953538929183026, 0.9997860252570635, 0.9997829892739375, 0.9995188224055067, 0.9996121359924149, 0.00013990783896762772, 0.9998513711821515, 0.9994360498634562, 0.6647145375025554, 0.10263723958105833, 0.23270119380535384, 0.9992660014855149, 0.0012297257171963024, 0.7498594151081454, 0.24895113963685145, 0.9999698042522648, 0.9997310754330919, 0.9997516063272289, 0.9995511580748254, 0.9999386945439184, 0.4831127897448722, 0.5168929075220868, 0.1448663745593526, 0.09007532992403312, 0.7650665761382048, 0.9996730595057192, 0.028331386348474832, 0.971498552152093, 0.9994665156020565, 0.9996002137262469, 0.0014787913802565382, 0.9981841816731634, 0.4798336978626719, 0.2776813361096703, 0.2424693879260587, 0.008353887375305711, 0.5084786852904525, 0.4831737060565167, 0.9174394166759368, 0.08255595804272028, 0.9995398625828249, 0.005589326361852227, 0.9943411597735113, 0.6292039658726178, 0.1889131713960541, 0.18189161989283792, 0.10242331149072768, 0.8972016052012445, 0.606948319279124, 0.1981608431997821, 0.19493136400812985, 0.9999028094098328, 0.9997367729964145, 0.5170252004782088, 0.2825343864767073, 0.2004466931084748, 0.26184199157287114, 0.4565732205997543, 0.2815791568200027, 0.0006653689305798077, 0.9990514492655813, 0.0006657290563998259, 0.9992593136561386, 0.9997700661090809, 0.31967853521285133, 0.44652094113743546, 0.23379065407841146, 0.08309028582739462, 0.00016819895916476643, 0.9166843274479771, 0.9998725200310762, 0.9991652557303305, 0.9995074396423962, 0.8022823053703808, 0.1060582059315404, 0.09154927987924874, 0.0003358475219363805, 0.9994822252826685, 0.0003358478730799736, 0.9994832702860016, 0.9993062825843947, 0.0002478665929650659, 0.9996459694281108, 0.999669557599716, 0.9993410512850976, 0.8027565848571748, 0.1033881555858984, 0.09383598903720126, 0.9996959006681005, 0.9996087636020823, 0.9996437322866304, 0.23845790913551085, 0.433476109043246, 0.32806190299315774, 0.37112485422808356, 0.3984144324883961, 0.2304527775938434, 0.8680755722314365, 0.07041879794228219, 0.06153714775136371, 0.9986630771537452, 0.0009511076925273765, 0.5144010424114779, 0.3160179950216943, 0.16958650816711032, 0.2951882221762937, 0.43954774831839305, 0.26526831142535445, 0.2221967595675176, 0.5635380211315454, 0.21425379276775028, 0.9672708200248616, 0.032709641256879377, 0.9993003455894137, 0.3912939898367638, 0.35057108591710057, 0.25814779962986495, 0.9996238657932728, 0.9996661839232746, 0.030155329491340616, 0.9437501266734377, 0.026060161288812878, 0.9165958323501529, 0.08270484952721276, 0.0007437486468274528, 0.9996769641617396, 0.4439586335668476, 0.29260977174716396, 0.263410919789379, 0.9697543167707348, 0.029623618898472013, 0.7153765327773233, 0.14423386376440256, 0.14039370222915898, 0.9997181428672902, 0.9997003543646085, 0.9998699291324705, 0.9997128268214405, 0.9995903063673446, 0.9991860503642158, 0.999754766419703, 0.7331121712013292, 0.0461532979441434, 0.22074463073856013, 0.8178543498528313, 0.09147181920887565, 0.09070153020501143, 0.5809269685529078, 0.2492656518480174, 0.16978588122749483, 0.999548162304217, 0.6392910227595116, 0.15295987542150458, 0.20775311148419714, 0.3089266310894445, 0.41313238156234217, 0.27793627508943175, 0.9995449630426695, 0.13410855611305483, 0.8659842599518762, 0.9996853066994981, 0.9997871658942551, 0.9998621401316482, 0.9997207402323202, 0.9997921613162383, 0.9995804188476558, 0.9997329671393147, 0.9518638691067476, 0.011868626796842238, 0.036555370534274094, 0.9995164151580074, 0.9997560951678047, 0.9966738160395746, 0.0030359950880230733, 0.9982531757060912, 0.0008688017195005145, 0.9997905646051566, 0.000379298025033272, 0.9994502959626717, 0.999931991807245, 0.9996076439294511, 0.9998534161861897, 0.9995714456641976, 0.8218422642107327, 0.1189866906966243, 0.059238009102182056, 0.04686711137404832, 0.08475011194156981, 0.8684640446309273, 0.49791903446635627, 0.5019707372056897, 0.018732141348308636, 0.6238193321931532, 0.35747169739688983, 0.9993407944552223, 0.17397453326371537, 0.8260225278115338, 0.0008222736680031142, 0.5584151843150038, 0.44078436792011383, 0.03519827419621588, 0.8143426807244076, 0.15035784520774828, 0.9999900875051182, 0.999611873417579, 0.9998030523085413, 0.0002270443041066178, 0.9999031152855448, 0.6158538730919104, 0.21292577427085127, 0.17124159082361176, 0.999794774949036, 0.9995080399945098, 0.9992788184066701, 0.0007291640478076886, 0.2692575590404744, 0.3908566595111131, 0.3398841505253616, 0.6346653013857484, 0.17752148648405738, 0.1879163685813672, 0.3692053433428294, 0.3490586171172939, 0.2817399201279828, 0.9994207390140424, 0.07860212579377927, 0.2022566895425296, 0.7192094510130803, 0.9996684191952843, 0.9998515385227603, 0.4084311274065089, 0.3456098523167064, 0.2459004196372268, 0.4484591289867069, 0.3147193364083596, 0.2368357720665444, 0.7846148190587587, 0.08973714986748609, 0.12563200981448053, 0.9995591058661922], \"Term\": [\"add\", \"add\", \"add\", \"almond\", \"also\", \"also\", \"also\", \"aroma\", \"arrive\", \"arrive\", \"arrive\", \"baby\", \"baby\", \"bag\", \"bag\", \"bag\", \"bake\", \"bar\", \"best\", \"best\", \"best\", \"beverage\", \"bitter\", \"black\", \"black\", \"black\", \"blend\", \"blend\", \"box\", \"box\", \"box\", \"bread\", \"brew\", \"butter\", \"buy\", \"buy\", \"buy\", \"caffeine\", \"cake\", \"calorie\", \"calorie\", \"cat\", \"cat food\", \"cereal\", \"cereal\", \"chai\", \"cheese\", \"chew\", \"chew\", \"chip\", \"chocolate\", \"chocolate\", \"coffee\", \"condition\", \"cook\", \"cook\", \"cookie\", \"cooky\", \"cracker\", \"crunchy\", \"cup\", \"cup\", \"cup coffee\", \"day\", \"day\", \"day\", \"decaf\", \"delicious\", \"delicious\", \"delicious\", \"dog\", \"dog food\", \"dog love\", \"door\", \"drink\", \"eat\", \"eat\", \"energy\", \"energy\", \"energy\", \"espresso\", \"fat\", \"fat\", \"fee\", \"feed\", \"fiber\", \"fiber\", \"find\", \"find\", \"find\", \"flavor\", \"flavor\", \"flavor\", \"food\", \"food\", \"formula\", \"french\", \"french\", \"get\", \"get\", \"get\", \"ginger\", \"ginger\", \"give\", \"give\", \"give\", \"gluten\", \"gluten free\", \"go\", \"go\", \"go\", \"good\", \"good\", \"good\", \"gp\", \"gp\", \"gp product\", \"gp product\", \"granola\", \"great\", \"great\", \"great\", \"green\", \"green\", \"green\", \"green tea\", \"greenies\", \"hair\", \"happy\", \"happy\", \"happy\", \"href\", \"href\", \"href gp\", \"href gp\", \"iced\", \"juice\", \"juice\", \"keurig\", \"kibble\", \"last\", \"last\", \"last\", \"lb\", \"leaf\", \"lick\", \"like\", \"like\", \"like\", \"little\", \"little\", \"little\", \"local\", \"local\", \"local\", \"local store\", \"local store\", \"love\", \"love\", \"love\", \"make\", \"make\", \"make\", \"mix\", \"mix\", \"mix\", \"month\", \"month\", \"month old\", \"much\", \"much\", \"much\", \"noodle\", \"nut\", \"oil\", \"oil\", \"oil\", \"old\", \"old\", \"old\", \"olive\", \"one\", \"one\", \"one\", \"online\", \"online\", \"order\", \"order\", \"order\", \"pasta\", \"peanut\", \"peanut butter\", \"pepper\", \"pet\", \"pod\", \"popcorn\", \"price\", \"price\", \"price\", \"problem\", \"problem\", \"problem\", \"product\", \"product\", \"product\", \"puppy\", \"purchase\", \"purchase\", \"purchase\", \"really\", \"really\", \"really\", \"refreshing\", \"rice\", \"rice\", \"roast\", \"salad\", \"salt\", \"salty\", \"sauce\", \"science\", \"seller\", \"send\", \"send\", \"send\", \"service\", \"shampoo\", \"ship\", \"ship\", \"shipment\", \"shipment\", \"shipping\", \"smooth\", \"smooth\", \"snack\", \"soda\", \"sodium\", \"starbucks\", \"store\", \"store\", \"store\", \"strong\", \"strong\", \"strong\", \"sugar\", \"sugar\", \"sweet\", \"sweet\", \"sweet\", \"sweetener\", \"syrup\", \"syrup\", \"taste\", \"taste\", \"taste\", \"tasty\", \"tasty\", \"tasty\", \"tea\", \"tea bag\", \"teeth\", \"texture\", \"texture\", \"time\", \"time\", \"time\", \"toy\", \"training\", \"treat\", \"treat\", \"try\", \"try\", \"try\", \"two\", \"two\", \"two\", \"use\", \"use\", \"use\", \"vet\", \"water\", \"water\", \"water\", \"weak\", \"week\", \"well\", \"well\", \"well\", \"would\", \"would\", \"would\", \"year\", \"year\", \"year\", \"year old\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 3, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el23214116091239606037839869\", ldavis_el23214116091239606037839869_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el23214116091239606037839869\", ldavis_el23214116091239606037839869_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el23214116091239606037839869\", ldavis_el23214116091239606037839869_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=                 x           y  topics  cluster       Freq\n",
       "topic                                                     \n",
       "1      -632.045471 -384.325073       1        1  39.011081\n",
       "2     -1209.665405  212.164124       2        1  34.511685\n",
       "0     -1437.453247 -586.322510       3        1  26.477234, topic_info=     Category          Freq       Term         Total  loglift  logprob\n",
       "term                                                                  \n",
       "8722  Default  27293.000000        tea  27293.000000  30.0000  30.0000\n",
       "1637  Default  25392.000000     coffee  25392.000000  29.0000  29.0000\n",
       "8593  Default  43781.000000      taste  43781.000000  28.0000  28.0000\n",
       "2439  Default  16172.000000      drink  16172.000000  27.0000  27.0000\n",
       "2359  Default  22656.000000        dog  22656.000000  26.0000  26.0000\n",
       "2014  Default  14295.000000        cup  14295.000000  25.0000  25.0000\n",
       "3198  Default  36988.000000     flavor  36988.000000  24.0000  24.0000\n",
       "3347  Default  26491.000000       food  26491.000000  23.0000  23.0000\n",
       "9124  Default  12342.000000      treat  12342.000000  22.0000  22.0000\n",
       "1521  Default  13011.000000  chocolate  13011.000000  21.0000  21.0000\n",
       "717   Default   8845.000000        bar   8845.000000  20.0000  20.0000\n",
       "7977  Default   8364.000000      snack   8364.000000  19.0000  19.0000\n",
       "1491  Default   8313.000000       chip   8313.000000  18.0000  18.0000\n",
       "1334  Default   8855.000000        cat   8855.000000  17.0000  17.0000\n",
       "2555  Default  22764.000000        eat  22764.000000  16.0000  16.0000\n",
       "7533  Default   6826.000000       salt   6826.000000  15.0000  15.0000\n",
       "4092  Default   5945.000000      green   5945.000000  14.0000  14.0000\n",
       "9589  Default  10432.000000      water  10432.000000  13.0000  13.0000\n",
       "8337  Default   6678.000000     strong   6678.000000  12.0000  12.0000\n",
       "8400  Default  11353.000000      sugar  11353.000000  11.0000  11.0000\n",
       "7568  Default   5481.000000      sauce   5481.000000  10.0000  10.0000\n",
       "927   Default   4345.000000      blend   4345.000000   9.0000   9.0000\n",
       "4637  Default   4034.000000      juice   4034.000000   8.0000   8.0000\n",
       "2972  Default   5223.000000        fat   5223.000000   7.0000   7.0000\n",
       "4926  Default  52118.000000       like  52118.000000   6.0000   6.0000\n",
       "8846  Default   4404.000000    texture   4404.000000   5.0000   5.0000\n",
       "216   Default  12651.000000        add  12651.000000   4.0000   4.0000\n",
       "8492  Default  10249.000000      sweet  10249.000000   3.0000   3.0000\n",
       "6095  Default   5372.000000        oil   5372.000000   2.0000   2.0000\n",
       "6774  Default  15166.000000      price  15166.000000   1.0000   1.0000\n",
       "...       ...           ...        ...           ...      ...      ...\n",
       "3974   Topic3   3002.948370         gp   3005.851202   1.3279  -6.0227\n",
       "3474   Topic3   1778.822822     french   1789.124369   1.3231  -6.5464\n",
       "4092   Topic3   5450.476395      green   5945.340001   1.2420  -5.4266\n",
       "8337   Topic3   5799.610349     strong   6678.457255   1.1878  -5.3645\n",
       "3700   Topic3   2698.289623     ginger   3007.127923   1.2205  -6.1297\n",
       "9589   Topic3   7502.521949      water  10432.287826   0.9992  -5.1071\n",
       "914    Topic3   2937.924420      black   3528.565885   1.1457  -6.0446\n",
       "3198   Topic3  17871.672272     flavor  36988.767758   0.6015  -4.2391\n",
       "8593   Topic3  19297.691713      taste  43781.044439   0.5097  -4.1623\n",
       "4926   Topic3  17098.352601       like  52118.212581   0.2144  -4.2833\n",
       "8532   Topic3   2317.043434      syrup   2805.008244   1.1378  -6.2820\n",
       "8400   Topic3   5699.486768      sugar  11353.251450   0.6398  -5.3819\n",
       "2675   Topic3   2666.752511     energy   3485.971134   1.0610  -6.1415\n",
       "9176   Topic3  10082.117801        try  29663.048378   0.2497  -4.8116\n",
       "3842   Topic3  12797.367425       good  45447.255914   0.0616  -4.5731\n",
       "6121   Topic3   8904.309151        one  33802.698867  -0.0051  -4.9358\n",
       "9362   Topic3   8068.600471        use  28639.888860   0.0621  -5.0343\n",
       "5382   Topic3   8200.867707       make  30915.867621   0.0019  -5.0181\n",
       "9847   Topic3   7063.649382      would  29826.575345  -0.1116  -5.1674\n",
       "4015   Topic3   6815.596153      great  29154.287740  -0.1245  -5.2031\n",
       "3067   Topic3   5557.199739       find  22918.357024  -0.0880  -5.4072\n",
       "7173   Topic3   5121.151280     really  18425.086824   0.0486  -5.4889\n",
       "3579   Topic3   5983.686560        get  32898.711901  -0.3755  -5.3333\n",
       "6858   Topic3   5881.411213    product  34637.744655  -0.4443  -5.3505\n",
       "5242   Topic3   5470.822568       love  32260.821094  -0.4455  -5.4229\n",
       "1155   Topic3   4861.291918        buy  26786.491741  -0.3777  -5.5410\n",
       "5772   Topic3   4374.393957       much  16943.781842  -0.0252  -5.6466\n",
       "813    Topic3   4009.154469       best  13079.156102   0.1464  -5.7337\n",
       "216    Topic3   3940.471939        add  12651.346448   0.1624  -5.7510\n",
       "9662   Topic3   3973.216501       well  16156.946807  -0.0739  -5.7427\n",
       "\n",
       "[230 rows x 6 columns], token_table=      Topic      Freq      Term\n",
       "term                           \n",
       "216       1  0.047505       add\n",
       "216       2  0.641038       add\n",
       "216       3  0.311429       add\n",
       "319       2  0.999650    almond\n",
       "344       1  0.353761      also\n",
       "344       2  0.386341      also\n",
       "344       3  0.259984      also\n",
       "529       3  0.999639     aroma\n",
       "537       1  0.999208    arrive\n",
       "537       2  0.000824    arrive\n",
       "537       3  0.000275    arrive\n",
       "615       1  0.987341      baby\n",
       "615       2  0.012541      baby\n",
       "645       1  0.486694       bag\n",
       "645       2  0.319240       bag\n",
       "645       3  0.194036       bag\n",
       "697       2  0.999714      bake\n",
       "717       2  0.999950       bar\n",
       "813       1  0.264620      best\n",
       "813       2  0.428850      best\n",
       "813       3  0.306518      best\n",
       "854       3  0.999631  beverage\n",
       "909       3  0.999799    bitter\n",
       "914       1  0.081336     black\n",
       "914       2  0.086154     black\n",
       "914       3  0.832633     black\n",
       "927       2  0.000230     blend\n",
       "927       3  0.999868     blend\n",
       "1002      1  0.561415       box\n",
       "1002      2  0.262362       box\n",
       "...     ...       ...       ...\n",
       "8982      3  0.171242      time\n",
       "9096      1  0.999795       toy\n",
       "9110      1  0.999508  training\n",
       "9124      1  0.999279     treat\n",
       "9124      2  0.000729     treat\n",
       "9176      1  0.269258       try\n",
       "9176      2  0.390857       try\n",
       "9176      3  0.339884       try\n",
       "9271      1  0.634665       two\n",
       "9271      2  0.177521       two\n",
       "9271      3  0.187916       two\n",
       "9362      1  0.369205       use\n",
       "9362      2  0.349059       use\n",
       "9362      3  0.281740       use\n",
       "9494      1  0.999421       vet\n",
       "9589      1  0.078602     water\n",
       "9589      2  0.202257     water\n",
       "9589      3  0.719209     water\n",
       "9634      3  0.999668      weak\n",
       "9644      1  0.999852      week\n",
       "9662      1  0.408431      well\n",
       "9662      2  0.345610      well\n",
       "9662      3  0.245900      well\n",
       "9847      1  0.448459     would\n",
       "9847      2  0.314719     would\n",
       "9847      3  0.236836     would\n",
       "9931      1  0.784615      year\n",
       "9931      2  0.089737      year\n",
       "9931      3  0.125632      year\n",
       "9943      1  0.999559  year old\n",
       "\n",
       "[295 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 3, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda.best_estimator_, tf_dtm, vectorizer, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-negative matrix factorizaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Topics:  0.334\n",
      "3 Topics:  0.375\n",
      "4 Topics:  0.4\n",
      "5 Topics:  0.381\n",
      "6 Topics:  0.406\n",
      "7 Topics:  0.411\n",
      "8 Topics:  0.442\n",
      "9 Topics:  0.437\n",
      "10 Topics:  0.43\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "# transform sparse matrix into gensim corpus\n",
    "corpus_gensim = gensim.matutils.Sparse2Corpus(tf_dtm, documents_columns=False)\n",
    "id2word = corpora.Dictionary.from_corpus(corpus_gensim,id2word=dict((id, word) for word, id in vectorizer.vocabulary_.items()))\n",
    "\n",
    "score_list =[]\n",
    "for n in range(2,11):    \n",
    "    nmf = NMF(n_components=n, random_state =42)\n",
    "    nmf.fit(tf_dtm)\n",
    "    \n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in nmf.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:20]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    \n",
    "    text = [[item for sublist in [[keywords[i]]*v for i,v in enumerate(text) if v!=0] for item in sublist] for text in tf_dtm.toarray()]\n",
    "    \n",
    "    coherence_model = CoherenceModel(topics = topic_keywords, texts = text, dictionary=id2word, coherence='c_v', processes = -1)\n",
    "    score_list.append(coherence_model.get_coherence())\n",
    "    \n",
    "    \n",
    "for i in range(9):\n",
    "    print (i+2,'Topics: ',np.round(score_list[i],3))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>get</td>\n",
       "      <td>make</td>\n",
       "      <td>one</td>\n",
       "      <td>good</td>\n",
       "      <td>use</td>\n",
       "      <td>buy</td>\n",
       "      <td>would</td>\n",
       "      <td>time</td>\n",
       "      <td>try</td>\n",
       "      <td>go</td>\n",
       "      <td>find</td>\n",
       "      <td>great</td>\n",
       "      <td>bag</td>\n",
       "      <td>love</td>\n",
       "      <td>eat</td>\n",
       "      <td>order</td>\n",
       "      <td>little</td>\n",
       "      <td>much</td>\n",
       "      <td>day</td>\n",
       "      <td>price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>food</td>\n",
       "      <td>cat</td>\n",
       "      <td>eat</td>\n",
       "      <td>diet</td>\n",
       "      <td>chicken</td>\n",
       "      <td>cat food</td>\n",
       "      <td>day</td>\n",
       "      <td>ingredient</td>\n",
       "      <td>would</td>\n",
       "      <td>dry</td>\n",
       "      <td>feed</td>\n",
       "      <td>like</td>\n",
       "      <td>year</td>\n",
       "      <td>fee</td>\n",
       "      <td>look</td>\n",
       "      <td>dog food</td>\n",
       "      <td>science</td>\n",
       "      <td>science diet</td>\n",
       "      <td>healthy</td>\n",
       "      <td>old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tea</td>\n",
       "      <td>green</td>\n",
       "      <td>green tea</td>\n",
       "      <td>drink</td>\n",
       "      <td>bag</td>\n",
       "      <td>flavor</td>\n",
       "      <td>cup</td>\n",
       "      <td>tea bag</td>\n",
       "      <td>leaf</td>\n",
       "      <td>make</td>\n",
       "      <td>black</td>\n",
       "      <td>use</td>\n",
       "      <td>organic</td>\n",
       "      <td>brew</td>\n",
       "      <td>find</td>\n",
       "      <td>black tea</td>\n",
       "      <td>good</td>\n",
       "      <td>water</td>\n",
       "      <td>taste</td>\n",
       "      <td>strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coffee</td>\n",
       "      <td>cup</td>\n",
       "      <td>drink</td>\n",
       "      <td>roast</td>\n",
       "      <td>blend</td>\n",
       "      <td>strong</td>\n",
       "      <td>brew</td>\n",
       "      <td>flavor</td>\n",
       "      <td>cup coffee</td>\n",
       "      <td>keurig</td>\n",
       "      <td>good</td>\n",
       "      <td>try</td>\n",
       "      <td>bean</td>\n",
       "      <td>use</td>\n",
       "      <td>bold</td>\n",
       "      <td>espresso</td>\n",
       "      <td>dark</td>\n",
       "      <td>taste</td>\n",
       "      <td>starbucks</td>\n",
       "      <td>french</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dog</td>\n",
       "      <td>treat</td>\n",
       "      <td>food</td>\n",
       "      <td>love</td>\n",
       "      <td>dog food</td>\n",
       "      <td>give</td>\n",
       "      <td>like</td>\n",
       "      <td>get</td>\n",
       "      <td>chew</td>\n",
       "      <td>dog love</td>\n",
       "      <td>small</td>\n",
       "      <td>one</td>\n",
       "      <td>would</td>\n",
       "      <td>bag</td>\n",
       "      <td>toy</td>\n",
       "      <td>large</td>\n",
       "      <td>pet</td>\n",
       "      <td>old</td>\n",
       "      <td>teeth</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>like</td>\n",
       "      <td>taste</td>\n",
       "      <td>flavor</td>\n",
       "      <td>good</td>\n",
       "      <td>drink</td>\n",
       "      <td>try</td>\n",
       "      <td>would</td>\n",
       "      <td>really</td>\n",
       "      <td>taste like</td>\n",
       "      <td>sweet</td>\n",
       "      <td>chip</td>\n",
       "      <td>sugar</td>\n",
       "      <td>think</td>\n",
       "      <td>add</td>\n",
       "      <td>much</td>\n",
       "      <td>juice</td>\n",
       "      <td>water</td>\n",
       "      <td>one</td>\n",
       "      <td>calorie</td>\n",
       "      <td>bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chocolate</td>\n",
       "      <td>bar</td>\n",
       "      <td>dark</td>\n",
       "      <td>dark chocolate</td>\n",
       "      <td>magnesium</td>\n",
       "      <td>sugar</td>\n",
       "      <td>good</td>\n",
       "      <td>taste</td>\n",
       "      <td>blue</td>\n",
       "      <td>almonds</td>\n",
       "      <td>diamond</td>\n",
       "      <td>milk</td>\n",
       "      <td>blue diamond</td>\n",
       "      <td>rda</td>\n",
       "      <td>one</td>\n",
       "      <td>find</td>\n",
       "      <td>healthy</td>\n",
       "      <td>body</td>\n",
       "      <td>cocoa</td>\n",
       "      <td>snack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>product</td>\n",
       "      <td>use</td>\n",
       "      <td>gp</td>\n",
       "      <td>gp product</td>\n",
       "      <td>href</td>\n",
       "      <td>href gp</td>\n",
       "      <td>pack</td>\n",
       "      <td>ounce</td>\n",
       "      <td>oil</td>\n",
       "      <td>great</td>\n",
       "      <td>purchase</td>\n",
       "      <td>find</td>\n",
       "      <td>also</td>\n",
       "      <td>make</td>\n",
       "      <td>ingredient</td>\n",
       "      <td>organic</td>\n",
       "      <td>sugar</td>\n",
       "      <td>price</td>\n",
       "      <td>order</td>\n",
       "      <td>quality</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1          2               3          4         5      6   \\\n",
       "0        get   make        one            good        use       buy  would   \n",
       "1       food    cat        eat            diet    chicken  cat food    day   \n",
       "2        tea  green  green tea           drink        bag    flavor    cup   \n",
       "3     coffee    cup      drink           roast      blend    strong   brew   \n",
       "4        dog  treat       food            love   dog food      give   like   \n",
       "5       like  taste     flavor            good      drink       try  would   \n",
       "6  chocolate    bar       dark  dark chocolate  magnesium     sugar   good   \n",
       "7    product    use         gp      gp product       href   href gp   pack   \n",
       "\n",
       "           7           8         9         10     11            12    13  \\\n",
       "0        time         try        go      find  great           bag  love   \n",
       "1  ingredient       would       dry      feed   like          year   fee   \n",
       "2     tea bag        leaf      make     black    use       organic  brew   \n",
       "3      flavor  cup coffee    keurig      good    try          bean   use   \n",
       "4         get        chew  dog love     small    one         would   bag   \n",
       "5      really  taste like     sweet      chip  sugar         think   add   \n",
       "6       taste        blue   almonds   diamond   milk  blue diamond   rda   \n",
       "7       ounce         oil     great  purchase   find          also  make   \n",
       "\n",
       "           14         15       16            17         18       19  \n",
       "0         eat      order   little          much        day    price  \n",
       "1        look   dog food  science  science diet    healthy      old  \n",
       "2        find  black tea     good         water      taste   strong  \n",
       "3        bold   espresso     dark         taste  starbucks   french  \n",
       "4         toy      large      pet           old      teeth       go  \n",
       "5        much      juice    water           one    calorie      bit  \n",
       "6         one       find  healthy          body      cocoa    snack  \n",
       "7  ingredient    organic    sugar         price      order  quality  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=8, random_state =42)\n",
    "nmf.fit(tf_dtm)\n",
    "\n",
    "import numpy as np\n",
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer=vectorizer, model=nmf, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, model=nmf, n_words=20)        \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#003366\"><b> NMF gave the best result (max coherence score) with 8 topics. Coherence score is the average of the pointwise mutual information of every pair of unique words and measures how much words in the topic co-occur together in the documents.\n",
    "    \n",
    "<span style=\"color:#003366\"><b>Topic 1: Unclear <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 2: Cat food <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 3: Tea <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 4: Coffee <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 5: Dog food <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 6: Chip(s) <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 7: Chocolate <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 8: Unclear <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize your bags\n",
    "In the above exercise, you may find it important to normalize your data.  One useful method when dealing with text is *Term Frequency - Inverse Document Frequency (TF-IDF)*. You can see more detail on this here: http://blog.christianperone.com/2011/10/machine-learning-text-feature-extraction-tf-idf-part-ii/.\n",
    "\n",
    "Once you understand the concept, **express your data as TF-IDF vectors (instead of simple bag-of-words counts), and see if it changes your above story**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transform = TfidfTransformer()\n",
    "\n",
    "tfidf_dtm = tfidf_transform.fit_transform(tf_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierachical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10000)\n"
     ]
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "tfidf_dtm2 = tfidf_transform.fit_transform(tf_dtm2)\n",
    "print(tf_dtm2.shape)\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "Z2 = linkage(tfidf_dtm2.toarray(),method='complete',metric='correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "f,ax = plt.subplots(figsize=(20, 12))\n",
    "dendrogram(Z2,truncate_mode='level',p=3,leaf_font_size=10)\n",
    "plt.ylim((1.004,1.01))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>811</td>\n",
       "      <td>sauce</td>\n",
       "      <td>good</td>\n",
       "      <td>like</td>\n",
       "      <td>taste</td>\n",
       "      <td>great</td>\n",
       "      <td>flavor</td>\n",
       "      <td>make</td>\n",
       "      <td>love</td>\n",
       "      <td>get</td>\n",
       "      <td>...</td>\n",
       "      <td>product</td>\n",
       "      <td>coconut</td>\n",
       "      <td>buy</td>\n",
       "      <td>try</td>\n",
       "      <td>soup</td>\n",
       "      <td>would</td>\n",
       "      <td>one</td>\n",
       "      <td>water</td>\n",
       "      <td>chicken</td>\n",
       "      <td>order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1131</td>\n",
       "      <td>food</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>bar</td>\n",
       "      <td>eat</td>\n",
       "      <td>treat</td>\n",
       "      <td>love</td>\n",
       "      <td>like</td>\n",
       "      <td>one</td>\n",
       "      <td>...</td>\n",
       "      <td>get</td>\n",
       "      <td>good</td>\n",
       "      <td>product</td>\n",
       "      <td>snack</td>\n",
       "      <td>would</td>\n",
       "      <td>great</td>\n",
       "      <td>try</td>\n",
       "      <td>taste</td>\n",
       "      <td>give</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1595</td>\n",
       "      <td>chocolate</td>\n",
       "      <td>chip</td>\n",
       "      <td>taste</td>\n",
       "      <td>like</td>\n",
       "      <td>good</td>\n",
       "      <td>great</td>\n",
       "      <td>love</td>\n",
       "      <td>flavor</td>\n",
       "      <td>product</td>\n",
       "      <td>...</td>\n",
       "      <td>popcorn</td>\n",
       "      <td>use</td>\n",
       "      <td>would</td>\n",
       "      <td>one</td>\n",
       "      <td>try</td>\n",
       "      <td>cooky</td>\n",
       "      <td>buy</td>\n",
       "      <td>get</td>\n",
       "      <td>eat</td>\n",
       "      <td>sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3663</td>\n",
       "      <td>good</td>\n",
       "      <td>like</td>\n",
       "      <td>product</td>\n",
       "      <td>taste</td>\n",
       "      <td>flavor</td>\n",
       "      <td>great</td>\n",
       "      <td>buy</td>\n",
       "      <td>love</td>\n",
       "      <td>get</td>\n",
       "      <td>...</td>\n",
       "      <td>use</td>\n",
       "      <td>one</td>\n",
       "      <td>try</td>\n",
       "      <td>price</td>\n",
       "      <td>would</td>\n",
       "      <td>find</td>\n",
       "      <td>order</td>\n",
       "      <td>eat</td>\n",
       "      <td>salt</td>\n",
       "      <td>tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>505</td>\n",
       "      <td>drink</td>\n",
       "      <td>juice</td>\n",
       "      <td>energy</td>\n",
       "      <td>cracker</td>\n",
       "      <td>soda</td>\n",
       "      <td>flavor</td>\n",
       "      <td>syrup</td>\n",
       "      <td>box</td>\n",
       "      <td>taste</td>\n",
       "      <td>...</td>\n",
       "      <td>good</td>\n",
       "      <td>cherry</td>\n",
       "      <td>water</td>\n",
       "      <td>product</td>\n",
       "      <td>would</td>\n",
       "      <td>orange</td>\n",
       "      <td>great</td>\n",
       "      <td>sugar</td>\n",
       "      <td>one</td>\n",
       "      <td>energy drink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2295</td>\n",
       "      <td>coffee</td>\n",
       "      <td>tea</td>\n",
       "      <td>dog</td>\n",
       "      <td>like</td>\n",
       "      <td>good</td>\n",
       "      <td>cup</td>\n",
       "      <td>love</td>\n",
       "      <td>flavor</td>\n",
       "      <td>taste</td>\n",
       "      <td>...</td>\n",
       "      <td>get</td>\n",
       "      <td>find</td>\n",
       "      <td>great</td>\n",
       "      <td>try</td>\n",
       "      <td>make</td>\n",
       "      <td>drink</td>\n",
       "      <td>use</td>\n",
       "      <td>buy</td>\n",
       "      <td>treat</td>\n",
       "      <td>would</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      n          0      1        2        3       4       5      6       7  \\\n",
       "0   811      sauce   good     like    taste   great  flavor   make    love   \n",
       "1  1131       food    cat      dog      bar     eat   treat   love    like   \n",
       "2  1595  chocolate   chip    taste     like    good   great   love  flavor   \n",
       "3  3663       good   like  product    taste  flavor   great    buy    love   \n",
       "4   505      drink  juice   energy  cracker    soda  flavor  syrup     box   \n",
       "5  2295     coffee    tea      dog     like    good     cup   love  flavor   \n",
       "\n",
       "         8      ...            10       11       12       13     14      15  \\\n",
       "0      get      ...       product  coconut      buy      try   soup   would   \n",
       "1      one      ...           get     good  product    snack  would   great   \n",
       "2  product      ...       popcorn      use    would      one    try   cooky   \n",
       "3      get      ...           use      one      try    price  would    find   \n",
       "4    taste      ...          good   cherry    water  product  would  orange   \n",
       "5    taste      ...           get     find    great      try   make   drink   \n",
       "\n",
       "      16     17       18            19  \n",
       "0    one  water  chicken         order  \n",
       "1    try  taste     give           buy  \n",
       "2    buy    get      eat         sweet  \n",
       "3  order    eat     salt           tea  \n",
       "4  great  sugar      one  energy drink  \n",
       "5    use    buy    treat         would  \n",
       "\n",
       "[6 rows x 21 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_df2 = pd.DataFrame(tfidf_dtm2.toarray(), columns = vectorizer2.get_feature_names())\n",
    "tf_df2['Cluster'] = fcluster(Z2, 6, criterion='maxclust')\n",
    "\n",
    "n_topic1 = tf_df2.loc[tf_df2['Cluster']==1,:].shape[0]\n",
    "n_topic2 = tf_df2.loc[tf_df2['Cluster']==2,:].shape[0]\n",
    "n_topic3 = tf_df2.loc[tf_df2['Cluster']==3,:].shape[0]\n",
    "n_topic4 = tf_df2.loc[tf_df2['Cluster']==4,:].shape[0]\n",
    "n_topic5 = tf_df2.loc[tf_df2['Cluster']==5,:].shape[0]\n",
    "n_topic6 = tf_df2.loc[tf_df2['Cluster']==6,:].shape[0]\n",
    "n_topics = [n_topic1,n_topic2,n_topic3,n_topic4,n_topic5,n_topic6]\n",
    "number = pd.DataFrame(n_topics)\n",
    "number.columns = ['n']\n",
    "\n",
    "tf_df_sum2 = tf_df2.groupby('Cluster').sum()\n",
    "top20_topic1 = pd.DataFrame(tf_df_sum2.iloc[0,:-1].sort_values(ascending=False)[:20].index).T\n",
    "top20_topic2 = pd.DataFrame(tf_df_sum2.iloc[1,:-1].sort_values(ascending=False)[:20].index).T\n",
    "top20_topic3 = pd.DataFrame(tf_df_sum2.iloc[2,:-1].sort_values(ascending=False)[:20].index).T\n",
    "top20_topic4 = pd.DataFrame(tf_df_sum2.iloc[3,:-1].sort_values(ascending=False)[:20].index).T\n",
    "top20_topic5 = pd.DataFrame(tf_df_sum2.iloc[4,:-1].sort_values(ascending=False)[:20].index).T\n",
    "top20_topic6 = pd.DataFrame(tf_df_sum2.iloc[5,:-1].sort_values(ascending=False)[:20].index).T\n",
    "\n",
    "top20 = pd.concat([top20_topic1,top20_topic2,top20_topic3,top20_topic4,top20_topic5,top20_topic6],axis=0).reset_index(drop=True)\n",
    "\n",
    "summary = pd.concat([number,top20],axis=1)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#003366\"><b> 10000 reviews were sampled due to memory resource constraints. Correlation metric(mean-centered cosine distance) was used as the distance metric to compare the similarity of the reivews. Complete linkage was used as it gave the most balanced dendrogram. The dendrogram was cut to yield 6 clusters or topics and the top 20 words with the highest tfidf score is shown in the table above. The topics are numbered according to the position of the branch on the dendrogram from left to right.\n",
    "    \n",
    "<span style=\"color:#003366\"><b>Topic 1: Unclear <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 2: Pet food <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 3: Snacks like chocolate, chip(s) and popcorn <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 4: Unclear <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 5: Sugary drinks <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 6: Coffee and tea\n",
    "    \n",
    "<span style=\"color:#003366\"><b> Unlike LDA and NMF performed below, each document can only have 1 topic. However, the hierarachical relationship of the topics can inferred from the dendrogram. For example,topic 1-4 can merge together to form a food topic and topic 5,6 can merge to form a drink topic (approximately). In LDA and NMF, the topic were assumed to be independent of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# parameters = {'n_components':[3,4,5,7,10]}\n",
    "# lda0 = LatentDirichletAllocation(random_state =42)\n",
    "# lda_tfidf = GridSearchCV(lda0, parameters, cv= 3,n_jobs=-1) # default scoring of LDA, max likelihood\n",
    "# lda_tfidf.fit(tfidf_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator LatentDirichletAllocation from version 0.19.2 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator GridSearchCV from version 0.19.2 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "#pickle.dump(lda_tfidf, open('lda_tfidf.pickle', 'wb'))\n",
    "lda_tfidf = pickle.load(open('lda_tfidf.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'n_components': 3}, -1557167.8433135194),\n",
       " ({'n_components': 4}, -1581874.1606186759),\n",
       " ({'n_components': 5}, -1605622.863494779),\n",
       " ({'n_components': 7}, -1666598.3143369574),\n",
       " ({'n_components': 10}, -1700993.3340613218)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(lda_tfidf.cv_results_['params'],lda_tfidf.cv_results_['mean_test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tea</td>\n",
       "      <td>coffee</td>\n",
       "      <td>drink</td>\n",
       "      <td>taste</td>\n",
       "      <td>flavor</td>\n",
       "      <td>cup</td>\n",
       "      <td>like</td>\n",
       "      <td>good</td>\n",
       "      <td>water</td>\n",
       "      <td>try</td>\n",
       "      <td>strong</td>\n",
       "      <td>sugar</td>\n",
       "      <td>one</td>\n",
       "      <td>use</td>\n",
       "      <td>great</td>\n",
       "      <td>make</td>\n",
       "      <td>green</td>\n",
       "      <td>blend</td>\n",
       "      <td>would</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>food</td>\n",
       "      <td>treat</td>\n",
       "      <td>product</td>\n",
       "      <td>get</td>\n",
       "      <td>love</td>\n",
       "      <td>cat</td>\n",
       "      <td>one</td>\n",
       "      <td>would</td>\n",
       "      <td>eat</td>\n",
       "      <td>give</td>\n",
       "      <td>like</td>\n",
       "      <td>buy</td>\n",
       "      <td>time</td>\n",
       "      <td>use</td>\n",
       "      <td>order</td>\n",
       "      <td>go</td>\n",
       "      <td>old</td>\n",
       "      <td>day</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good</td>\n",
       "      <td>taste</td>\n",
       "      <td>great</td>\n",
       "      <td>like</td>\n",
       "      <td>flavor</td>\n",
       "      <td>love</td>\n",
       "      <td>bar</td>\n",
       "      <td>buy</td>\n",
       "      <td>find</td>\n",
       "      <td>snack</td>\n",
       "      <td>chip</td>\n",
       "      <td>eat</td>\n",
       "      <td>make</td>\n",
       "      <td>chocolate</td>\n",
       "      <td>store</td>\n",
       "      <td>product</td>\n",
       "      <td>salt</td>\n",
       "      <td>try</td>\n",
       "      <td>use</td>\n",
       "      <td>delicious</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0       1      2        3       4     5     6     7      8      9   \\\n",
       "0   tea  coffee  drink    taste  flavor   cup  like  good  water    try   \n",
       "1   dog    food  treat  product     get  love   cat   one  would    eat   \n",
       "2  good   taste  great     like  flavor  love   bar   buy   find  snack   \n",
       "\n",
       "       10     11    12         13     14       15     16     17     18  \\\n",
       "0  strong  sugar   one        use  great     make  green  blend  would   \n",
       "1    give   like   buy       time    use    order     go    old    day   \n",
       "2    chip    eat  make  chocolate  store  product   salt    try    use   \n",
       "\n",
       "          19  \n",
       "0       love  \n",
       "1       good  \n",
       "2  delicious  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=lda_tfidf.best_estimator_, n_words=20)        \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#003366\"><b> LDA using tf-idf vectors again gave the best results (maximum cross-validated log likelihood) with 3 topics. The topics here seemed more well-defined. Topic 1 now includes green (tea), topic 2 include cat and topic 3 include salt, delicious etc.\n",
    "       \n",
    "<span style=\"color:#003366\"><b>Topic 1: Tea and cofee <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 2: Pet food <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 3: Snacks like chocolate and chip(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el23214116431026964068399334\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el23214116431026964068399334_data = {\"mdsDat\": {\"x\": [-632.0454711914062, -1209.6654052734375, -1437.4532470703125], \"y\": [-384.3250732421875, 212.16412353515625, -586.322509765625], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [37.145737253099846, 34.06585810700189, 28.788404639898264]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"Freq\": [2116.0, 1987.0, 1703.0, 1179.0, 1109.0, 1589.0, 1054.0, 2346.0, 845.0, 752.0, 818.0, 802.0, 2133.0, 626.0, 778.0, 666.0, 443.0, 537.0, 530.0, 855.0, 1446.0, 470.0, 367.0, 1072.0, 410.0, 334.0, 333.0, 411.0, 321.0, 319.0, 844.8607614851787, 801.4491516816315, 817.6736410888517, 665.5113144903677, 529.4106337324973, 469.56454073209295, 411.0386023200513, 386.41010029408994, 400.10325406776553, 357.78119717113395, 376.4156711851664, 383.9879161539997, 352.69956458774305, 306.90212131759984, 284.21070221976476, 337.3897541519046, 353.41816075374356, 256.5356747178142, 259.9908764101802, 241.11788208127334, 267.15107453414254, 249.73636907365668, 223.3854987956782, 202.20171575882424, 350.18888508249495, 282.9597839288832, 170.77994504672873, 167.04322485075187, 153.48953694340045, 176.3719962185191, 206.57793995627654, 193.27450306312318, 417.23189327691097, 397.65675283935605, 433.5775042687411, 399.42642279167467, 372.20487707380175, 390.2531477061341, 300.3640054231698, 710.4049284390522, 728.737328357213, 569.0202312299657, 457.5682378910633, 1133.574189861869, 1218.2979402991502, 1143.8749006693638, 826.0433507647722, 788.0069564358654, 954.3096321556868, 994.3202797891819, 826.8268889258669, 893.338728892006, 774.6994660860599, 484.1642129198855, 658.3853836728135, 703.6754096226404, 564.7887841358602, 508.4577485095409, 590.841595282769, 540.1452630782751, 537.4453981651628, 483.8592812061074, 552.584866421654, 522.0333757056658, 541.1614253699687, 536.4841408726052, 1703.0088673281148, 752.292225932417, 410.0795304652893, 366.61845151209894, 314.19877548146553, 293.36511044495677, 294.8430979774845, 220.71466997341622, 240.8881343637289, 209.9274554347815, 193.52447630036053, 189.6750344796992, 188.20791378875106, 184.807619907132, 187.12158061888005, 173.66965405626735, 170.1088998955658, 149.611415516967, 193.84361984392888, 150.977197627925, 144.17422302245143, 138.35448123542778, 126.03185409123996, 126.69567191943464, 129.27177902135867, 117.61410208470792, 140.27099120325428, 123.11167518927923, 109.75014384925983, 121.88459934630258, 139.01078259933647, 1018.9438962082294, 1422.8114151910208, 509.6273396148867, 437.96637728099057, 202.10876858326742, 320.21734129648877, 367.02882816304134, 345.05261174087735, 228.7607859269117, 286.4200292121209, 941.5304776611339, 618.2846576531201, 1015.4139262056789, 361.0669081172943, 504.6865536614445, 415.2092593085223, 322.2472801747442, 452.6201009701635, 875.2220120076888, 461.08406028276244, 658.4811512306475, 751.0732601890301, 572.1014943776031, 662.747215527286, 404.507733173558, 539.1829067836651, 372.12725557546264, 523.3457894342349, 585.4775492766885, 562.3618998512135, 475.3939634507526, 599.1665136639806, 407.5991743131343, 483.8042967245174, 431.6598019737022, 414.4507996151681, 423.6997677781472, 2115.6484382540793, 1987.0627130249457, 1179.1830625699843, 1108.91626576926, 321.26936482884975, 318.8847535002738, 333.5202331150051, 443.07269032252583, 366.62046821408234, 332.76468897770957, 307.0889632767612, 317.5249336712057, 255.9143086873415, 246.36835970473223, 281.4029348040565, 217.83701684289784, 210.2114746317176, 212.13934128999233, 190.0064774370399, 219.095840939286, 183.54716705302056, 183.82365957372934, 181.115278274762, 175.74727505290878, 174.90800810400074, 199.53603644557356, 175.06669387508998, 201.2487426779306, 171.4586682291792, 211.51396144502792, 270.01930991260735, 258.4523246984518, 198.56647384435004, 592.0583161840211, 494.2754184583395, 670.4362394887139, 300.0192727472832, 358.56919473862723, 306.4482860323107, 250.01760214168758, 1120.4321205186122, 568.7460143924221, 1150.5364526479702, 343.1510867101705, 903.9664537742319, 280.4736004289283, 634.7075649294625, 741.5073830707556, 342.81619064453935, 528.7299089698788, 544.3913189292159, 496.14434830851326, 497.4298526731016, 424.1742500698081, 346.10924612361106, 383.50730378284595, 390.3058126175559, 392.26775618858653, 350.42048034400773, 355.7883813374793, 351.1418766986567, 349.88802188014637, 343.2869758325194], \"Term\": [\"tea\", \"coffee\", \"dog\", \"drink\", \"cup\", \"food\", \"treat\", \"taste\", \"bar\", \"cat\", \"snack\", \"chip\", \"flavor\", \"strong\", \"water\", \"salt\", \"blend\", \"green\", \"sauce\", \"sugar\", \"eat\", \"cooky\", \"juice\", \"chocolate\", \"chew\", \"bitter\", \"roast\", \"cheese\", \"green tea\", \"brew\", \"bar\", \"chip\", \"snack\", \"salt\", \"sauce\", \"cooky\", \"cheese\", \"popcorn\", \"gluten\", \"pasta\", \"peanut\", \"butter\", \"nut\", \"gluten free\", \"salty\", \"grocery store\", \"almond\", \"bread\", \"peanut butter\", \"cracker\", \"crunchy\", \"cookie\", \"granola\", \"salad\", \"noodle\", \"pepper\", \"bake\", \"vinegar\", \"chili\", \"microwave\", \"cake\", \"lunch\", \"cereal\", \"grocery\", \"candy\", \"texture\", \"cook\", \"fat\", \"soup\", \"store\", \"chocolate\", \"delicious\", \"tasty\", \"great\", \"good\", \"taste\", \"find\", \"eat\", \"flavor\", \"like\", \"buy\", \"love\", \"make\", \"free\", \"try\", \"product\", \"best\", \"add\", \"use\", \"really\", \"order\", \"sweet\", \"would\", \"price\", \"get\", \"one\", \"dog\", \"cat\", \"chew\", \"dog love\", \"dog food\", \"teeth\", \"baby\", \"pet\", \"year old\", \"hair\", \"vet\", \"toy\", \"greenies\", \"feed\", \"fee\", \"puppy\", \"bone\", \"cat food\", \"gum\", \"formula\", \"training\", \"shampoo\", \"love treat\", \"month old\", \"lick\", \"breath\", \"within\", \"skin\", \"dent\", \"plant\", \"picture\", \"treat\", \"food\", \"old\", \"month\", \"picky\", \"week\", \"problem\", \"open\", \"clean\", \"away\", \"get\", \"give\", \"product\", \"last\", \"day\", \"two\", \"help\", \"take\", \"love\", \"year\", \"eat\", \"one\", \"time\", \"would\", \"small\", \"order\", \"seem\", \"go\", \"buy\", \"use\", \"bag\", \"like\", \"work\", \"good\", \"make\", \"price\", \"great\", \"tea\", \"coffee\", \"drink\", \"cup\", \"green tea\", \"brew\", \"bitter\", \"blend\", \"juice\", \"roast\", \"syrup\", \"smooth\", \"soda\", \"keurig\", \"bold\", \"cup coffee\", \"chai\", \"starbucks\", \"iced\", \"french\", \"decaf\", \"refreshing\", \"pod\", \"espresso\", \"caffeine\", \"aroma\", \"sweetener\", \"orange\", \"weak\", \"cocoa\", \"vanilla\", \"ice\", \"aftertaste\", \"strong\", \"green\", \"water\", \"ginger\", \"coconut\", \"black\", \"lemon\", \"flavor\", \"sugar\", \"taste\", \"dark\", \"like\", \"energy\", \"try\", \"good\", \"milk\", \"use\", \"one\", \"make\", \"great\", \"would\", \"hot\", \"best\", \"really\", \"love\", \"sweet\", \"find\", \"buy\", \"get\", \"chocolate\"], \"Total\": [2116.0, 1987.0, 1703.0, 1179.0, 1109.0, 1589.0, 1054.0, 2346.0, 845.0, 752.0, 818.0, 802.0, 2133.0, 626.0, 778.0, 666.0, 443.0, 537.0, 530.0, 855.0, 1446.0, 470.0, 367.0, 1072.0, 410.0, 334.0, 333.0, 411.0, 321.0, 319.0, 845.5826812868796, 802.1445908157933, 818.5294208848679, 666.2277136655098, 530.1295948136774, 470.29736204537386, 411.7703584556213, 387.10221577716555, 400.84638787519543, 358.4698064157941, 377.1556734713858, 384.7631486122442, 353.44082113198226, 307.5935337687095, 284.91439409980364, 338.2426118697586, 354.3172766693869, 257.2390078081996, 260.72797454310887, 241.8225147868963, 267.9321406101681, 250.47008479826025, 224.0877563784949, 202.90846075697658, 351.4361683598863, 284.11395397666786, 171.51533835543785, 167.76876330456787, 154.17927950845674, 177.2533016275703, 207.7143204370554, 194.33833914493445, 424.8068426563887, 408.6320070981175, 449.51010128120566, 421.6150695737585, 391.62367015984705, 416.05612197141033, 311.1027819179321, 984.0206464913718, 1072.3751821816638, 764.2030665689325, 567.251343180023, 2054.7038103131176, 2443.609620094423, 2346.763734050519, 1495.6573391915106, 1446.8560285784647, 2133.651677825274, 2497.4532472273945, 1763.4463149012122, 2160.8284970882814, 1702.5036163682753, 689.0651517484881, 1679.6793856178783, 2044.3798054972626, 1121.1258011567156, 897.8921704778493, 1681.9334041038612, 1271.9210624596462, 1289.5773866153434, 841.8544889039563, 1639.506332018748, 1257.9677139120379, 1832.579924911249, 1831.948719990851, 1703.6782771007333, 752.9603667226872, 410.8031386455567, 367.2873961932812, 314.86391230722376, 294.09600032035326, 295.58113146415, 221.38351906392415, 241.6866075159509, 210.63755656032856, 194.18993938827748, 190.34132774467588, 188.87244308013047, 185.50214557986845, 187.8410991165123, 174.33908925027148, 170.7941981097582, 150.27672724957682, 194.72685496379188, 151.68072935791585, 144.84681896196847, 139.02393749882398, 126.69821451656408, 127.37750963099441, 129.98546824819215, 118.30580427838768, 141.10028715637392, 123.87322818929081, 110.45390389770516, 122.67366025044194, 139.96863527962753, 1054.1610912144113, 1589.65231255716, 586.3826864914769, 497.9711136525067, 213.66472746155628, 359.88900953226266, 468.69007029502876, 442.64104466131255, 267.2878433077324, 360.7334561745528, 1832.579924911249, 1035.7484801224825, 2044.3798054972626, 499.01902757769415, 864.5124253247272, 646.4679998936112, 441.91705966571976, 755.9494748707921, 2160.8284970882814, 799.4096375661611, 1446.8560285784647, 1831.948719990851, 1192.940117662743, 1639.506332018748, 679.5913074236156, 1289.5773866153434, 596.3545951083712, 1215.1728072808999, 1763.4463149012122, 1681.9334041038612, 1156.0517464433965, 2497.4532472273945, 762.163654245448, 2443.609620094423, 1702.5036163682753, 1257.9677139120379, 2054.7038103131176, 2116.3065873009496, 1987.719825022062, 1179.8631025455236, 1109.6799730887492, 321.9173261974185, 319.533421396943, 334.201739848315, 443.99649274873275, 367.3928555244373, 333.48992673382435, 307.82256197882333, 318.36552078742426, 256.5924523126736, 247.02143107570907, 282.17089375359, 218.48591582757604, 210.86797431711173, 212.8035155744795, 190.65631028714404, 219.8618103402918, 184.19618143192508, 184.4945393706344, 181.77858928897714, 176.40018669347046, 175.56115727151703, 200.28206501635532, 175.75839225914342, 202.04822801593272, 172.17524124498397, 212.3993597882017, 271.41937902010136, 259.7195269881808, 199.41658129407696, 626.3048476829313, 537.6841923276281, 778.2999393938256, 318.94050572941495, 392.860155342505, 341.38323394563884, 269.57576310193764, 2133.651677825274, 855.4451395475488, 2346.763734050519, 428.2244044821538, 2497.4532472273945, 350.27677895268374, 1679.6793856178783, 2443.609620094423, 558.0678465122794, 1681.9334041038612, 1831.948719990851, 1702.5036163682753, 2054.7038103131176, 1639.506332018748, 642.061924078693, 1121.1258011567156, 1271.9210624596462, 2160.8284970882814, 841.8544889039563, 1495.6573391915106, 1763.4463149012122, 1832.579924911249, 1072.3751821816638], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9895, 0.9895, 0.9893, 0.9892, 0.989, 0.9888, 0.9885, 0.9885, 0.9885, 0.9884, 0.9884, 0.9883, 0.9882, 0.9881, 0.9878, 0.9878, 0.9878, 0.9876, 0.9875, 0.9874, 0.9874, 0.9874, 0.9872, 0.9868, 0.9868, 0.9863, 0.986, 0.986, 0.9858, 0.9853, 0.9848, 0.9848, 0.9723, 0.9631, 0.9542, 0.9363, 0.9395, 0.9263, 0.9552, 0.6645, 0.604, 0.6954, 0.7754, 0.3956, 0.2943, 0.2717, 0.3966, 0.3827, 0.1857, 0.0694, 0.2329, 0.107, 0.2029, 0.6374, 0.0538, -0.0762, 0.3047, 0.4217, -0.0558, 0.1339, 0.1151, 0.4365, -0.0972, 0.1108, -0.2294, -0.2378, 1.0765, 1.076, 1.0751, 1.0751, 1.0748, 1.0744, 1.0744, 1.0738, 1.0736, 1.0735, 1.0734, 1.0734, 1.0733, 1.0731, 1.073, 1.073, 1.0729, 1.0724, 1.0723, 1.0722, 1.0722, 1.072, 1.0716, 1.0715, 1.0714, 1.071, 1.071, 1.0707, 1.0705, 1.0704, 1.07, 1.0429, 0.966, 0.9366, 0.9485, 1.0213, 0.9601, 0.8324, 0.8278, 0.9212, 0.8462, 0.4109, 0.5609, 0.3771, 0.7533, 0.5386, 0.6341, 0.7611, 0.564, 0.1731, 0.5266, 0.2897, 0.1852, 0.342, 0.1711, 0.5581, 0.2049, 0.6053, 0.2345, -0.0257, -0.0187, 0.1883, -0.3506, 0.451, -0.5427, -0.2953, -0.0334, -0.502, 1.2449, 1.2449, 1.2446, 1.2445, 1.2432, 1.2432, 1.2432, 1.2431, 1.2431, 1.243, 1.2428, 1.2426, 1.2426, 1.2426, 1.2425, 1.2422, 1.2421, 1.2421, 1.2418, 1.2417, 1.2417, 1.2416, 1.2415, 1.2415, 1.2415, 1.2415, 1.2413, 1.2412, 1.241, 1.241, 1.24, 1.2403, 1.2409, 1.189, 1.161, 1.096, 1.184, 1.1539, 1.1372, 1.1699, 0.6011, 0.837, 0.5324, 1.0237, 0.229, 1.023, 0.272, 0.0527, 0.7579, 0.088, 0.0317, 0.0122, -0.1732, -0.1068, 0.6273, 0.1725, 0.0638, -0.4611, 0.3687, -0.1908, -0.3686, -0.4107, 0.1061], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.4598, -5.5125, -5.4925, -5.6984, -5.9272, -6.0471, -6.1802, -6.242, -6.2072, -6.319, -6.2682, -6.2483, -6.3333, -6.4724, -6.5492, -6.3777, -6.3313, -6.6517, -6.6383, -6.7136, -6.6111, -6.6785, -6.79, -6.8897, -6.3405, -6.5536, -7.0586, -7.0807, -7.1653, -7.0263, -6.8682, -6.9348, -6.1653, -6.2133, -6.1269, -6.2089, -6.2795, -6.2321, -6.4939, -5.6331, -5.6076, -5.855, -6.073, -5.1658, -5.0937, -5.1567, -5.4823, -5.5294, -5.3379, -5.2969, -5.4813, -5.404, -5.5465, -6.0165, -5.7091, -5.6426, -5.8625, -5.9675, -5.8174, -5.9071, -5.9121, -6.0171, -5.8843, -5.9412, -5.9052, -5.9139, -4.6722, -5.4892, -6.096, -6.2081, -6.3623, -6.431, -6.4259, -6.7155, -6.628, -6.7656, -6.847, -6.8671, -6.8748, -6.8931, -6.8806, -6.9552, -6.9759, -7.1043, -6.8453, -7.0952, -7.1414, -7.1826, -7.2758, -7.2706, -7.2505, -7.345, -7.1688, -7.2993, -7.4142, -7.3093, -7.1778, -5.1859, -4.852, -5.8787, -6.0302, -6.8036, -6.3434, -6.2069, -6.2687, -6.6797, -6.4549, -5.2649, -5.6854, -5.1893, -6.2233, -5.8884, -6.0836, -6.3371, -5.9973, -5.3379, -5.9788, -5.6224, -5.4909, -5.7631, -5.616, -6.1097, -5.8223, -6.1931, -5.8521, -5.7399, -5.7802, -5.9482, -5.7168, -6.1021, -5.9307, -6.0447, -6.0854, -6.0633, -4.2869, -4.3496, -4.8715, -4.9329, -6.1718, -6.1792, -6.1343, -5.8503, -6.0397, -6.1366, -6.2169, -6.1835, -6.3992, -6.4372, -6.3043, -6.5603, -6.5959, -6.5868, -6.697, -6.5545, -6.7316, -6.7301, -6.7449, -6.775, -6.7798, -6.6481, -6.7789, -6.6395, -6.7997, -6.5898, -6.3456, -6.3893, -6.6529, -5.5604, -5.741, -5.4361, -6.2402, -6.0619, -6.219, -6.4225, -4.9226, -5.6006, -4.8961, -6.1059, -5.1373, -6.3076, -5.4909, -5.3354, -6.1069, -5.6736, -5.6444, -5.7372, -5.7346, -5.8939, -6.0973, -5.9947, -5.9771, -5.9721, -6.0849, -6.0697, -6.0829, -6.0864, -6.1055]}, \"token.table\": {\"Topic\": [1, 2, 3, 1, 3, 1, 3, 3, 1, 2, 3, 2, 1, 2, 3, 1, 1, 1, 2, 3, 3, 1, 2, 3, 1, 3, 3, 2, 1, 2, 3, 1, 1, 2, 3, 3, 1, 2, 1, 2, 3, 2, 2, 1, 2, 3, 1, 2, 1, 1, 1, 3, 2, 3, 1, 3, 1, 3, 3, 1, 2, 1, 1, 1, 1, 3, 3, 1, 3, 1, 2, 3, 3, 1, 3, 2, 2, 2, 2, 3, 1, 2, 2, 3, 3, 1, 2, 2, 2, 1, 2, 3, 1, 2, 3, 1, 2, 2, 1, 2, 3, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 1, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 2, 3, 3, 2, 1, 2, 3, 1, 2, 3, 2, 1, 2, 3, 1, 2, 3, 1, 3, 3, 3, 3, 1, 2, 3, 1, 3, 2, 1, 2, 3, 1, 2, 3, 2, 1, 2, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 2, 1, 2, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 3, 1, 2, 3, 1, 1, 1, 1, 3, 2, 2, 3, 2, 3, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 1, 2, 3, 3, 3, 1, 1, 1, 1, 1, 2, 3, 2, 2, 1, 2, 3, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 3, 1, 2, 3, 3, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 3, 2, 1, 2, 1, 2, 3, 2, 2, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 2, 1, 1, 2, 3, 3, 1, 2, 3, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2], \"Freq\": [0.5657694951607023, 0.05568597393313999, 0.3775509032666891, 0.005014628139298574, 0.9979109997204163, 0.996282211576671, 0.002822329211265357, 0.99859166113385, 0.07484750731558194, 0.7928291515650532, 0.13029010532712412, 0.9980339358562186, 0.41434131428255505, 0.41088126155368193, 0.1747326628080921, 0.9969953803526895, 0.9993109115172595, 0.5039577176950742, 0.15430917727654483, 0.34251285592019204, 0.9993963530877888, 0.002929259262214494, 0.09959481491529279, 0.8963533342376352, 0.0022522700434165857, 0.9977556292335473, 0.9958504091686632, 0.9953499702065534, 0.999070872608956, 0.9974151371502611, 0.9983306240874241, 0.9980165756128239, 0.4689680615802179, 0.3317367787477962, 0.19904206724867773, 0.9968036365205251, 0.9965610438627804, 0.0048143045597235775, 0.9654955445116843, 0.017797152894224594, 0.017797152894224594, 0.9987245454540093, 0.9981585488675353, 0.9816226061530196, 0.016478077321513518, 0.9958838020807919, 0.9981291551472753, 0.9980449549431275, 0.9923512451723965, 0.9985730866618083, 0.679799394943947, 0.3198507441231465, 0.8567542659856362, 0.14216883016355536, 0.004708112119533553, 0.9981197693411132, 0.08654479090748711, 0.9138111745819962, 0.9996378639418894, 0.9498915115323919, 0.04851596429869744, 0.9981231898466483, 0.9993677148345451, 0.9965986840074791, 0.9965209824844256, 0.999387234963918, 0.9977759855790452, 0.1984940585130579, 0.8009819067056336, 0.13880656481591427, 0.584144293600306, 0.2764564082583626, 0.9989349321446298, 0.7445664966442204, 0.2551677800450316, 0.9958905581270759, 0.9996018748904355, 0.9972562358737993, 0.9992175168648314, 0.9992684722967763, 0.5446291714139724, 0.45477918120608357, 0.1969870803491678, 0.799367862286478, 0.9977313703518588, 0.9373735402619534, 0.06008804745268932, 0.9955222838853248, 0.9972930470518345, 0.552265534595311, 0.2099411354272732, 0.2380224337965263, 0.4471207788575712, 0.027652123640038467, 0.5249216690990354, 0.10442535055541023, 0.8951643002430648, 0.9955120906868166, 0.7024009250385981, 0.20462506287281473, 0.09287946116212867, 0.9960802181199275, 0.295212226569709, 0.514029422234133, 0.19098757726321286, 0.059572238893103646, 0.9406142983121628, 0.1911661023886663, 0.5966699559403827, 0.21240678043185143, 0.9978884981858464, 0.9980703958193223, 0.35797377738674596, 0.4303914610879727, 0.21149255353653726, 0.49844295503834835, 0.19806764387402348, 0.30364915651761454, 0.9951458464483993, 0.5519043641755786, 0.2063557763760541, 0.2418840114596672, 0.0799725947193157, 0.9187549253800454, 0.9971504292476139, 0.9953807815163362, 0.9739814627502621, 0.02447189604900156, 0.0024471896049001562, 0.9963262704752378, 0.9962673101050852, 0.005135398505696316, 0.9969732056773742, 0.09051472244646379, 0.7286435156940335, 0.17876657683176597, 0.451669829847217, 0.00934489303132173, 0.5388888314728865, 0.0038503073357495665, 0.9933792926233881, 0.9965576262010128, 0.9989306936198404, 0.9958650102897508, 0.16833025467535248, 0.72341930878336, 0.10821230657701231, 0.07048111366308299, 0.9273830745142498, 0.992418627547577, 0.3980054485918854, 0.23984432968464722, 0.3619687379547932, 0.41326741164480124, 0.4049372734481535, 0.18141189850477277, 0.9944891526748959, 0.9931133550341997, 0.005145665051990672, 0.4552119552340244, 0.2537439544014175, 0.2913356513497756, 0.9929293185736894, 0.005641643855532326, 0.23653038035599414, 0.1487274361329357, 0.614620609561409, 0.060244458317986985, 0.87956909144261, 0.060244458317986985, 0.9970362928896315, 0.9959134304059005, 0.0028454669440168587, 0.9987527724427234, 0.10232225708947855, 0.8697391852605677, 0.02899130617535226, 0.29258460902916367, 0.40994597272556327, 0.29695154349228553, 0.07455250794749502, 0.7794125830874479, 0.14458668207999034, 0.9948119910467611, 0.416415490511526, 0.41796638619313314, 0.16517039009116394, 0.9986894114723593, 0.9969358184095474, 0.9972079154744152, 0.9960792000495712, 0.003519714487807672, 0.9982676259481926, 0.9454063962726132, 0.051482526529696766, 0.9930796261770188, 0.007144457742280711, 0.9945085175654933, 0.9957168262113675, 0.9971526492687398, 0.41495500578204847, 0.3291022459650729, 0.25517348056712175, 0.12801636689727924, 0.7830334441883581, 0.08961145682809547, 0.3443587136338217, 0.49648308854876283, 0.15897241751561372, 0.9980550015964308, 0.42455464882053745, 0.2680983986070431, 0.3066228019259437, 0.997319490472068, 0.9985309099479476, 0.9955228049457009, 0.9996582044534639, 0.9967906356479718, 0.997869210048395, 0.18445401595339514, 0.6237899448605726, 0.19116143471533678, 0.9926348115493949, 0.9929506302366122, 0.2707509616295102, 0.5959464101084327, 0.1339040081972034, 0.9988518832487884, 0.999353204819082, 0.0012217031843754058, 0.9976910766184516, 0.9643115312261625, 0.03214371770753875, 0.9962241433262494, 0.7215295761644626, 0.2205238282080118, 0.05792561386109066, 0.054286662678384064, 0.9452265972236283, 0.3343288619902236, 0.6651507778756546, 0.5749212083315476, 0.009502829889777646, 0.41574880767777206, 0.9956850296057259, 0.9973278047796902, 0.2209142350797239, 0.5992463981503888, 0.17990620341821828, 0.48747983591234967, 0.02215817435965226, 0.49046266707614905, 0.8074022309624554, 0.035257739343338666, 0.158659827045024, 0.9998551309612751, 0.9962733246315509, 0.9463608604013568, 0.05218029806724273, 0.3378208126570293, 0.47948760506159, 0.18274177955144513, 0.9982067596736861, 0.9941536930666678, 0.03320175663064875, 0.9666454287608879, 0.39174142734266604, 0.23040111304196315, 0.3780483379370713, 0.16087416549174158, 0.6419497949910842, 0.1964521059370306, 0.35138133207770283, 0.33413927009757866, 0.31451899267192013, 0.0036843353028449006, 0.9947705317681231, 0.999021888626796, 0.9954177208592028, 0.005139406798766266, 0.13362457676792291, 0.8608506387933494, 0.9931741565367593, 0.0750231301452943, 0.8891630239442286, 0.033343613397908575, 0.992202091302943, 0.19418401700947277, 0.5353181009450331, 0.2715952129794653, 0.33729665399894065, 0.40439002097883847, 0.2586144327225151, 0.2601920094849812, 0.5766755594835401, 0.16262000592811326, 0.9971590998648712], \"Term\": [\"add\", \"add\", \"add\", \"aftertaste\", \"aftertaste\", \"almond\", \"almond\", \"aroma\", \"away\", \"away\", \"away\", \"baby\", \"bag\", \"bag\", \"bag\", \"bake\", \"bar\", \"best\", \"best\", \"best\", \"bitter\", \"black\", \"black\", \"black\", \"blend\", \"blend\", \"bold\", \"bone\", \"bread\", \"breath\", \"brew\", \"butter\", \"buy\", \"buy\", \"buy\", \"caffeine\", \"cake\", \"cake\", \"candy\", \"candy\", \"candy\", \"cat\", \"cat food\", \"cereal\", \"cereal\", \"chai\", \"cheese\", \"chew\", \"chili\", \"chip\", \"chocolate\", \"chocolate\", \"clean\", \"clean\", \"cocoa\", \"cocoa\", \"coconut\", \"coconut\", \"coffee\", \"cook\", \"cook\", \"cookie\", \"cooky\", \"cracker\", \"crunchy\", \"cup\", \"cup coffee\", \"dark\", \"dark\", \"day\", \"day\", \"day\", \"decaf\", \"delicious\", \"delicious\", \"dent\", \"dog\", \"dog food\", \"dog love\", \"drink\", \"eat\", \"eat\", \"energy\", \"energy\", \"espresso\", \"fat\", \"fat\", \"fee\", \"feed\", \"find\", \"find\", \"find\", \"flavor\", \"flavor\", \"flavor\", \"food\", \"food\", \"formula\", \"free\", \"free\", \"free\", \"french\", \"get\", \"get\", \"get\", \"ginger\", \"ginger\", \"give\", \"give\", \"give\", \"gluten\", \"gluten free\", \"go\", \"go\", \"go\", \"good\", \"good\", \"good\", \"granola\", \"great\", \"great\", \"great\", \"green\", \"green\", \"green tea\", \"greenies\", \"grocery\", \"grocery\", \"grocery\", \"grocery store\", \"gum\", \"gum\", \"hair\", \"help\", \"help\", \"help\", \"hot\", \"hot\", \"hot\", \"ice\", \"ice\", \"iced\", \"juice\", \"keurig\", \"last\", \"last\", \"last\", \"lemon\", \"lemon\", \"lick\", \"like\", \"like\", \"like\", \"love\", \"love\", \"love\", \"love treat\", \"lunch\", \"lunch\", \"make\", \"make\", \"make\", \"microwave\", \"microwave\", \"milk\", \"milk\", \"milk\", \"month\", \"month\", \"month\", \"month old\", \"noodle\", \"noodle\", \"nut\", \"old\", \"old\", \"old\", \"one\", \"one\", \"one\", \"open\", \"open\", \"open\", \"orange\", \"order\", \"order\", \"order\", \"pasta\", \"peanut\", \"peanut butter\", \"pepper\", \"pepper\", \"pet\", \"picky\", \"picky\", \"picture\", \"picture\", \"plant\", \"pod\", \"popcorn\", \"price\", \"price\", \"price\", \"problem\", \"problem\", \"problem\", \"product\", \"product\", \"product\", \"puppy\", \"really\", \"really\", \"really\", \"refreshing\", \"roast\", \"salad\", \"salt\", \"salty\", \"sauce\", \"seem\", \"seem\", \"seem\", \"shampoo\", \"skin\", \"small\", \"small\", \"small\", \"smooth\", \"snack\", \"snack\", \"soda\", \"soup\", \"soup\", \"starbucks\", \"store\", \"store\", \"store\", \"strong\", \"strong\", \"sugar\", \"sugar\", \"sweet\", \"sweet\", \"sweet\", \"sweetener\", \"syrup\", \"take\", \"take\", \"take\", \"taste\", \"taste\", \"taste\", \"tasty\", \"tasty\", \"tasty\", \"tea\", \"teeth\", \"texture\", \"texture\", \"time\", \"time\", \"time\", \"toy\", \"training\", \"treat\", \"treat\", \"try\", \"try\", \"try\", \"two\", \"two\", \"two\", \"use\", \"use\", \"use\", \"vanilla\", \"vanilla\", \"vet\", \"vinegar\", \"water\", \"water\", \"water\", \"weak\", \"week\", \"week\", \"week\", \"within\", \"work\", \"work\", \"work\", \"would\", \"would\", \"would\", \"year\", \"year\", \"year\", \"year old\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 2, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el23214116431026964068399334\", ldavis_el23214116431026964068399334_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el23214116431026964068399334\", ldavis_el23214116431026964068399334_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el23214116431026964068399334\", ldavis_el23214116431026964068399334_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=                 x           y  topics  cluster       Freq\n",
       "topic                                                     \n",
       "2      -632.045471 -384.325073       1        1  37.145737\n",
       "1     -1209.665405  212.164124       2        1  34.065858\n",
       "0     -1437.453247 -586.322510       3        1  28.788405, topic_info=     Category         Freq       Term        Total  loglift  logprob\n",
       "term                                                                \n",
       "8722  Default  2116.000000        tea  2116.000000  30.0000  30.0000\n",
       "1637  Default  1987.000000     coffee  1987.000000  29.0000  29.0000\n",
       "2359  Default  1703.000000        dog  1703.000000  28.0000  28.0000\n",
       "2439  Default  1179.000000      drink  1179.000000  27.0000  27.0000\n",
       "2014  Default  1109.000000        cup  1109.000000  26.0000  26.0000\n",
       "3347  Default  1589.000000       food  1589.000000  25.0000  25.0000\n",
       "9124  Default  1054.000000      treat  1054.000000  24.0000  24.0000\n",
       "8593  Default  2346.000000      taste  2346.000000  23.0000  23.0000\n",
       "717   Default   845.000000        bar   845.000000  22.0000  22.0000\n",
       "1334  Default   752.000000        cat   752.000000  21.0000  21.0000\n",
       "7977  Default   818.000000      snack   818.000000  20.0000  20.0000\n",
       "1491  Default   802.000000       chip   802.000000  19.0000  19.0000\n",
       "3198  Default  2133.000000     flavor  2133.000000  18.0000  18.0000\n",
       "8337  Default   626.000000     strong   626.000000  17.0000  17.0000\n",
       "9589  Default   778.000000      water   778.000000  16.0000  16.0000\n",
       "7533  Default   666.000000       salt   666.000000  15.0000  15.0000\n",
       "927   Default   443.000000      blend   443.000000  14.0000  14.0000\n",
       "4092  Default   537.000000      green   537.000000  13.0000  13.0000\n",
       "7568  Default   530.000000      sauce   530.000000  12.0000  12.0000\n",
       "8400  Default   855.000000      sugar   855.000000  11.0000  11.0000\n",
       "2555  Default  1446.000000        eat  1446.000000  10.0000  10.0000\n",
       "1870  Default   470.000000      cooky   470.000000   9.0000   9.0000\n",
       "4637  Default   367.000000      juice   367.000000   8.0000   8.0000\n",
       "1521  Default  1072.000000  chocolate  1072.000000   7.0000   7.0000\n",
       "1448  Default   410.000000       chew   410.000000   6.0000   6.0000\n",
       "909   Default   334.000000     bitter   334.000000   5.0000   5.0000\n",
       "7463  Default   333.000000      roast   333.000000   4.0000   4.0000\n",
       "1426  Default   411.000000     cheese   411.000000   3.0000   3.0000\n",
       "4096  Default   321.000000  green tea   321.000000   2.0000   2.0000\n",
       "1085  Default   319.000000       brew   319.000000   1.0000   1.0000\n",
       "...       ...          ...        ...          ...      ...      ...\n",
       "8337   Topic3   592.058316     strong   626.304848   1.1890  -5.5604\n",
       "4092   Topic3   494.275418      green   537.684192   1.1610  -5.7410\n",
       "9589   Topic3   670.436239      water   778.299939   1.0960  -5.4361\n",
       "3700   Topic3   300.019273     ginger   318.940506   1.1840  -6.2402\n",
       "1626   Topic3   358.569195    coconut   392.860155   1.1539  -6.0619\n",
       "914    Topic3   306.448286      black   341.383234   1.1372  -6.2190\n",
       "4864   Topic3   250.017602      lemon   269.575763   1.1699  -6.4225\n",
       "3198   Topic3  1120.432121     flavor  2133.651678   0.6011  -4.9226\n",
       "8400   Topic3   568.746014      sugar   855.445140   0.8370  -5.6006\n",
       "8593   Topic3  1150.536453      taste  2346.763734   0.5324  -4.8961\n",
       "2086   Topic3   343.151087       dark   428.224404   1.0237  -6.1059\n",
       "4926   Topic3   903.966454       like  2497.453247   0.2290  -5.1373\n",
       "2675   Topic3   280.473600     energy   350.276779   1.0230  -6.3076\n",
       "9176   Topic3   634.707565        try  1679.679386   0.2720  -5.4909\n",
       "3842   Topic3   741.507383       good  2443.609620   0.0527  -5.3354\n",
       "5645   Topic3   342.816191       milk   558.067847   0.7579  -6.1069\n",
       "9362   Topic3   528.729909        use  1681.933404   0.0880  -5.6736\n",
       "6121   Topic3   544.391319        one  1831.948720   0.0317  -5.6444\n",
       "5382   Topic3   496.144348       make  1702.503616   0.0122  -5.7372\n",
       "4015   Topic3   497.429853      great  2054.703810  -0.1732  -5.7346\n",
       "9847   Topic3   424.174250      would  1639.506332  -0.1068  -5.8939\n",
       "4360   Topic3   346.109246        hot   642.061924   0.6273  -6.0973\n",
       "813    Topic3   383.507304       best  1121.125801   0.1725  -5.9947\n",
       "7173   Topic3   390.305813     really  1271.921062   0.0638  -5.9771\n",
       "5242   Topic3   392.267756       love  2160.828497  -0.4611  -5.9721\n",
       "8492   Topic3   350.420480      sweet   841.854489   0.3687  -6.0849\n",
       "3067   Topic3   355.788381       find  1495.657339  -0.1908  -6.0697\n",
       "1155   Topic3   351.141877        buy  1763.446315  -0.3686  -6.0829\n",
       "3579   Topic3   349.888022        get  1832.579925  -0.4107  -6.0864\n",
       "1521   Topic3   343.286976  chocolate  1072.375182   0.1061  -6.1055\n",
       "\n",
       "[227 rows x 6 columns], token_table=      Topic      Freq        Term\n",
       "term                             \n",
       "216       1  0.565769         add\n",
       "216       2  0.055686         add\n",
       "216       3  0.377551         add\n",
       "279       1  0.005015  aftertaste\n",
       "279       3  0.997911  aftertaste\n",
       "319       1  0.996282      almond\n",
       "319       3  0.002822      almond\n",
       "529       3  0.998592       aroma\n",
       "606       1  0.074848        away\n",
       "606       2  0.792829        away\n",
       "606       3  0.130290        away\n",
       "615       2  0.998034        baby\n",
       "645       1  0.414341         bag\n",
       "645       2  0.410881         bag\n",
       "645       3  0.174733         bag\n",
       "697       1  0.996995        bake\n",
       "717       1  0.999311         bar\n",
       "813       1  0.503958        best\n",
       "813       2  0.154309        best\n",
       "813       3  0.342513        best\n",
       "909       3  0.999396      bitter\n",
       "914       1  0.002929       black\n",
       "914       2  0.099595       black\n",
       "914       3  0.896353       black\n",
       "927       1  0.002252       blend\n",
       "927       3  0.997756       blend\n",
       "959       3  0.995850        bold\n",
       "966       2  0.995350        bone\n",
       "1060      1  0.999071       bread\n",
       "1078      2  0.997415      breath\n",
       "...     ...       ...         ...\n",
       "9176      2  0.230401         try\n",
       "9176      3  0.378048         try\n",
       "9271      1  0.160874         two\n",
       "9271      2  0.641950         two\n",
       "9271      3  0.196452         two\n",
       "9362      1  0.351381         use\n",
       "9362      2  0.334139         use\n",
       "9362      3  0.314519         use\n",
       "9462      1  0.003684     vanilla\n",
       "9462      3  0.994771     vanilla\n",
       "9494      2  0.999022         vet\n",
       "9506      1  0.995418     vinegar\n",
       "9589      1  0.005139       water\n",
       "9589      2  0.133625       water\n",
       "9589      3  0.860851       water\n",
       "9634      3  0.993174        weak\n",
       "9644      1  0.075023        week\n",
       "9644      2  0.889163        week\n",
       "9644      3  0.033344        week\n",
       "9775      2  0.992202      within\n",
       "9813      1  0.194184        work\n",
       "9813      2  0.535318        work\n",
       "9813      3  0.271595        work\n",
       "9847      1  0.337297       would\n",
       "9847      2  0.404390       would\n",
       "9847      3  0.258614       would\n",
       "9931      1  0.260192        year\n",
       "9931      2  0.576676        year\n",
       "9931      3  0.162620        year\n",
       "9943      2  0.997159    year old\n",
       "\n",
       "[297 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 2, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda_tfidf.best_estimator_, tfidf_dtm, vectorizer, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-negative matrix factorizaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Topics:  0.38\n",
      "3 Topics:  0.484\n",
      "4 Topics:  0.5\n",
      "5 Topics:  0.526\n",
      "6 Topics:  0.553\n",
      "7 Topics:  0.551\n",
      "8 Topics:  0.594\n",
      "9 Topics:  0.577\n",
      "10 Topics:  0.585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "# transform sparse matrix into gensim corpus\n",
    "corpus_gensim = gensim.matutils.Sparse2Corpus(tfidf_dtm, documents_columns=False)\n",
    "id2word = corpora.Dictionary.from_corpus(corpus_gensim,id2word=dict((id, word) for word, id in vectorizer.vocabulary_.items()))\n",
    "\n",
    "score_list =[]\n",
    "for n in range(2,11):    \n",
    "    nmf = NMF(n_components=n, random_state =42)\n",
    "    nmf.fit(tfidf_dtm)\n",
    "    \n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in nmf.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:20]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    \n",
    "    text = [[item for sublist in [[keywords[i]]*v for i,v in enumerate(text) if v!=0] for item in sublist] for text in tf_dtm.toarray()]\n",
    "    \n",
    "    coherence_model = CoherenceModel(topics = topic_keywords, texts = text, dictionary=id2word, coherence='c_v', processes = -1)\n",
    "    score_list.append(coherence_model.get_coherence())\n",
    "    \n",
    "    \n",
    "for i in range(9):\n",
    "    print (i+2,'Topics: ',np.round(score_list[i],3))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>product</td>\n",
       "      <td>buy</td>\n",
       "      <td>order</td>\n",
       "      <td>great</td>\n",
       "      <td>price</td>\n",
       "      <td>find</td>\n",
       "      <td>store</td>\n",
       "      <td>get</td>\n",
       "      <td>box</td>\n",
       "      <td>time</td>\n",
       "      <td>use</td>\n",
       "      <td>love</td>\n",
       "      <td>purchase</td>\n",
       "      <td>shipping</td>\n",
       "      <td>good</td>\n",
       "      <td>local</td>\n",
       "      <td>go</td>\n",
       "      <td>make</td>\n",
       "      <td>one</td>\n",
       "      <td>grocery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coffee</td>\n",
       "      <td>cup</td>\n",
       "      <td>roast</td>\n",
       "      <td>strong</td>\n",
       "      <td>cup coffee</td>\n",
       "      <td>flavor</td>\n",
       "      <td>bold</td>\n",
       "      <td>blend</td>\n",
       "      <td>brew</td>\n",
       "      <td>keurig</td>\n",
       "      <td>drink</td>\n",
       "      <td>like</td>\n",
       "      <td>smooth</td>\n",
       "      <td>bitter</td>\n",
       "      <td>french</td>\n",
       "      <td>starbucks</td>\n",
       "      <td>dark</td>\n",
       "      <td>flavor coffee</td>\n",
       "      <td>try</td>\n",
       "      <td>taste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tea</td>\n",
       "      <td>green</td>\n",
       "      <td>green tea</td>\n",
       "      <td>drink</td>\n",
       "      <td>tea bag</td>\n",
       "      <td>bag</td>\n",
       "      <td>cup</td>\n",
       "      <td>black tea</td>\n",
       "      <td>iced</td>\n",
       "      <td>flavor</td>\n",
       "      <td>leaf</td>\n",
       "      <td>stash</td>\n",
       "      <td>black</td>\n",
       "      <td>earl</td>\n",
       "      <td>grey</td>\n",
       "      <td>chai</td>\n",
       "      <td>iced tea</td>\n",
       "      <td>earl grey</td>\n",
       "      <td>brew</td>\n",
       "      <td>love tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>treat</td>\n",
       "      <td>love</td>\n",
       "      <td>dog love</td>\n",
       "      <td>dog food</td>\n",
       "      <td>chew</td>\n",
       "      <td>give</td>\n",
       "      <td>food</td>\n",
       "      <td>teeth</td>\n",
       "      <td>get</td>\n",
       "      <td>small</td>\n",
       "      <td>toy</td>\n",
       "      <td>one</td>\n",
       "      <td>greenies</td>\n",
       "      <td>love treat</td>\n",
       "      <td>eat</td>\n",
       "      <td>training</td>\n",
       "      <td>dog treat</td>\n",
       "      <td>size</td>\n",
       "      <td>bone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>taste</td>\n",
       "      <td>like</td>\n",
       "      <td>flavor</td>\n",
       "      <td>good</td>\n",
       "      <td>drink</td>\n",
       "      <td>try</td>\n",
       "      <td>really</td>\n",
       "      <td>would</td>\n",
       "      <td>add</td>\n",
       "      <td>make</td>\n",
       "      <td>sugar</td>\n",
       "      <td>water</td>\n",
       "      <td>use</td>\n",
       "      <td>taste like</td>\n",
       "      <td>one</td>\n",
       "      <td>sweet</td>\n",
       "      <td>mix</td>\n",
       "      <td>little</td>\n",
       "      <td>think</td>\n",
       "      <td>sauce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>food</td>\n",
       "      <td>cat</td>\n",
       "      <td>eat</td>\n",
       "      <td>cat food</td>\n",
       "      <td>dry</td>\n",
       "      <td>cat love</td>\n",
       "      <td>dog food</td>\n",
       "      <td>chicken</td>\n",
       "      <td>diet</td>\n",
       "      <td>old</td>\n",
       "      <td>ingredient</td>\n",
       "      <td>feed</td>\n",
       "      <td>baby</td>\n",
       "      <td>fee</td>\n",
       "      <td>one</td>\n",
       "      <td>love</td>\n",
       "      <td>dry food</td>\n",
       "      <td>healthy</td>\n",
       "      <td>like</td>\n",
       "      <td>brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chip</td>\n",
       "      <td>salt</td>\n",
       "      <td>bag</td>\n",
       "      <td>potato</td>\n",
       "      <td>flavor</td>\n",
       "      <td>potato chip</td>\n",
       "      <td>snack</td>\n",
       "      <td>kettle</td>\n",
       "      <td>vinegar</td>\n",
       "      <td>eat</td>\n",
       "      <td>love</td>\n",
       "      <td>salt vinegar</td>\n",
       "      <td>salty</td>\n",
       "      <td>great</td>\n",
       "      <td>fat</td>\n",
       "      <td>healthy</td>\n",
       "      <td>crunchy</td>\n",
       "      <td>popchips</td>\n",
       "      <td>sea salt</td>\n",
       "      <td>sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>chocolate</td>\n",
       "      <td>bar</td>\n",
       "      <td>dark</td>\n",
       "      <td>dark chocolate</td>\n",
       "      <td>snack</td>\n",
       "      <td>granola</td>\n",
       "      <td>cooky</td>\n",
       "      <td>cocoa</td>\n",
       "      <td>milk</td>\n",
       "      <td>peanut</td>\n",
       "      <td>sweet</td>\n",
       "      <td>eat</td>\n",
       "      <td>almond</td>\n",
       "      <td>candy</td>\n",
       "      <td>butter</td>\n",
       "      <td>love</td>\n",
       "      <td>chocolate bar</td>\n",
       "      <td>peanut butter</td>\n",
       "      <td>hot chocolate</td>\n",
       "      <td>cookie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1          2               3           4            5   \\\n",
       "0    product    buy      order           great       price         find   \n",
       "1     coffee    cup      roast          strong  cup coffee       flavor   \n",
       "2        tea  green  green tea           drink     tea bag          bag   \n",
       "3        dog  treat       love        dog love    dog food         chew   \n",
       "4      taste   like     flavor            good       drink          try   \n",
       "5       food    cat        eat        cat food         dry     cat love   \n",
       "6       chip   salt        bag          potato      flavor  potato chip   \n",
       "7  chocolate    bar       dark  dark chocolate       snack      granola   \n",
       "\n",
       "         6          7        8       9           10            11        12  \\\n",
       "0     store        get      box    time         use          love  purchase   \n",
       "1      bold      blend     brew  keurig       drink          like    smooth   \n",
       "2       cup  black tea     iced  flavor        leaf         stash     black   \n",
       "3      give       food    teeth     get       small           toy       one   \n",
       "4    really      would      add    make       sugar         water       use   \n",
       "5  dog food    chicken     diet     old  ingredient          feed      baby   \n",
       "6     snack     kettle  vinegar     eat        love  salt vinegar     salty   \n",
       "7     cooky      cocoa     milk  peanut       sweet           eat    almond   \n",
       "\n",
       "           13          14         15             16             17  \\\n",
       "0    shipping        good      local             go           make   \n",
       "1      bitter      french  starbucks           dark  flavor coffee   \n",
       "2        earl        grey       chai       iced tea      earl grey   \n",
       "3    greenies  love treat        eat       training      dog treat   \n",
       "4  taste like         one      sweet            mix         little   \n",
       "5         fee         one       love       dry food        healthy   \n",
       "6       great         fat    healthy        crunchy       popchips   \n",
       "7       candy      butter       love  chocolate bar  peanut butter   \n",
       "\n",
       "              18        19  \n",
       "0            one   grocery  \n",
       "1            try     taste  \n",
       "2           brew  love tea  \n",
       "3           size      bone  \n",
       "4          think     sauce  \n",
       "5           like     brand  \n",
       "6       sea salt       sea  \n",
       "7  hot chocolate    cookie  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=8, random_state =42)\n",
    "nmf.fit(tfidf_dtm)\n",
    "\n",
    "import numpy as np\n",
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer=vectorizer, model=nmf, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, model=nmf, n_words=20)        \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#003366\"><b> NMF again gave the best result (max coherence score) with 8 topics using tf-idf vectors. The topics appear to be more well-defined compared to using tf vectors\n",
    "    \n",
    "<span style=\"color:#003366\"><b>Topic 1: Purchase <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 2: Coffee <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 3: Tea <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 4: Dog food <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 5: Sweet drink? <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 6: Dog food <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 7: Chip(s) <br/>\n",
    "<span style=\"color:#003366\"><b>Topic 8: Chocolate <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show us your bags (Version 2)\n",
    "\n",
    "Show and explain what one of your documents looks like as a TF-IDF vector below.  How is this different from a simple bag-of-words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>00 per</th>\n",
       "      <th>000</th>\n",
       "      <th>0g</th>\n",
       "      <th>10</th>\n",
       "      <th>10 00</th>\n",
       "      <th>10 12</th>\n",
       "      <th>10 15</th>\n",
       "      <th>10 day</th>\n",
       "      <th>10 gram</th>\n",
       "      <th>...</th>\n",
       "      <th>zip</th>\n",
       "      <th>zip lock</th>\n",
       "      <th>zipfizz</th>\n",
       "      <th>ziploc</th>\n",
       "      <th>ziploc bag</th>\n",
       "      <th>ziplock</th>\n",
       "      <th>ziplock bag</th>\n",
       "      <th>ziwipeak</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zukes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  00 per  000   0g   10  10 00  10 12  10 15  10 day  10 gram  ...    \\\n",
       "0  0.0     0.0  0.0  0.0  0.0    0.0    0.0    0.0     0.0      0.0  ...     \n",
       "1  0.0     0.0  0.0  0.0  0.0    0.0    0.0    0.0     0.0      0.0  ...     \n",
       "2  0.0     0.0  0.0  0.0  0.0    0.0    0.0    0.0     0.0      0.0  ...     \n",
       "3  0.0     0.0  0.0  0.0  0.0    0.0    0.0    0.0     0.0      0.0  ...     \n",
       "4  0.0     0.0  0.0  0.0  0.0    0.0    0.0    0.0     0.0      0.0  ...     \n",
       "\n",
       "   zip  zip lock  zipfizz  ziploc  ziploc bag  ziplock  ziplock bag  ziwipeak  \\\n",
       "0  0.0       0.0      0.0     0.0         0.0      0.0          0.0       0.0   \n",
       "1  0.0       0.0      0.0     0.0         0.0      0.0          0.0       0.0   \n",
       "2  0.0       0.0      0.0     0.0         0.0      0.0          0.0       0.0   \n",
       "3  0.0       0.0      0.0     0.0         0.0      0.0          0.0       0.0   \n",
       "4  0.0       0.0      0.0     0.0         0.0      0.0          0.0       0.0   \n",
       "\n",
       "   zoe  zukes  \n",
       "0  0.0    0.0  \n",
       "1  0.0    0.0  \n",
       "2  0.0    0.0  \n",
       "3  0.0    0.0  \n",
       "4  0.0    0.0  \n",
       "\n",
       "[5 rows x 10000 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tfidf_dtm.toarray(),columns = vectorizer.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#003366\"><b> In the tfidf document-term matrix, the term frequencies are normalized using the document frequency in of each document. Therefore, words that have high frequency in the document but does not appear in most of the documents will have high tfidf values and are considered to be important words in characterizing the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Simple Supervised Learning with Text\n",
    "Now that you are comfortable with treating text as numbers, we can try out supervised learning.  We'll use a labelled dataset of IMDB reviews to classify each review as 'positive' or 'negative'.  You can **find the data below:**\n",
    "\n",
    "http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "Load in and process the data, then train a supervised learning model.  **You should achieve val or test set accuracy of 85%**. Pretty good for a simple bag, no?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import wget\n",
    "import pickle\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start download large movie review dataset...\n",
      "100% [........................................................................] 84125825 / 84125825"
     ]
    }
   ],
   "source": [
    "# path  = 'data/sentiment.tar.gz'\n",
    "# if not os.path.exists(path):\n",
    "#     print('start download large movie review dataset...')\n",
    "#     wget.download('http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz', out=path)\n",
    "\n",
    "# tar = tarfile.open('data/sentiment.tar.gz', \"r:gz\")\n",
    "# tar.extractall('data/')\n",
    "# tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pos = []\n",
    "# for i in os.listdir('data/aclImdb/train/pos'):\n",
    "#     if i.endswith('.txt'):\n",
    "#         txt = open('data/aclImdb/train/pos/'+i,'r')\n",
    "#         train_pos.append(txt.read())\n",
    "#         txt.close()\n",
    "\n",
    "# train_pos_df = pd.DataFrame(train_pos,columns = ['Text'])\n",
    "# train_pos_df['label'] = 1\n",
    "\n",
    "# train_neg = []\n",
    "# for i in os.listdir('data/aclImdb/train/neg'):\n",
    "#     if i.endswith('.txt'):\n",
    "#         txt = open('data/aclImdb/train/neg/'+i,'r')\n",
    "#         train_neg.append(txt.read())\n",
    "#         txt.close()\n",
    "\n",
    "# train_neg_df = pd.DataFrame(train_neg,columns = ['Text'])\n",
    "# train_neg_df['label'] = 0\n",
    "\n",
    "# train = pd.concat([train_pos_df,train_neg_df],axis = 0)\n",
    "# train.to_csv('train.csv',index=False,header=True)\n",
    "\n",
    "# test_pos = []\n",
    "# for i in os.listdir('data/aclImdb/test/pos'):\n",
    "#     if i.endswith('.txt'):\n",
    "#         txt = open('data/aclImdb/test/pos/'+i,'r')\n",
    "#         test_pos.append(txt.read())\n",
    "#         txt.close()\n",
    "\n",
    "# test_pos_df = pd.DataFrame(test_pos,columns = ['Text'])\n",
    "# test_pos_df['label'] = 1\n",
    "\n",
    "# test_neg = []\n",
    "# for i in os.listdir('data/aclImdb/test/neg'):\n",
    "#     if i.endswith('.txt'):\n",
    "#         txt = open('data/aclImdb/test/neg/'+i,'r')\n",
    "#         test_neg.append(txt.read())\n",
    "#         txt.close()\n",
    "\n",
    "# test_neg_df = pd.DataFrame(test_neg,columns = ['Text'])\n",
    "# test_neg_df['label'] = 0\n",
    "\n",
    "# test = pd.concat([test_pos_df,test_neg_df],axis = 0)\n",
    "# test.to_csv('test.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import nltk\n",
    "# import string\n",
    "# import spacy\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "# from contractions import CONTRACTION_MAP\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# #function for tokenize text\n",
    "# def tokenize_text(text): \n",
    "#     tokens = nltk.word_tokenize(text) \n",
    "#     tokens = [token.strip() for token in tokens] #remove whitespace in tokens\n",
    "#     return tokens\n",
    "\n",
    "# #function for expand contractions\n",
    "# def expand_contractions(text, contraction_mapping): # contraction mapping is CONTRACTION_MAP from custom contraction.py file\n",
    "    \n",
    "#     contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), #from CONTRACTION_MAP in nltk\n",
    "#                                       flags=re.IGNORECASE|re.DOTALL)             # match all contraction keys in CONTRACTION_MAP\n",
    "#     def expand_match(contraction):\n",
    "#         match = contraction.group(0)\n",
    "#         first_char = match[0]\n",
    "#         expanded_contraction = contraction_mapping.get(match)\\\n",
    "#                                 if contraction_mapping.get(match)\\\n",
    "#                                 else contraction_mapping.get(match.lower()) #get key from dictionary + try lower case \n",
    "#         expanded_contraction = first_char+expanded_contraction[1:] #keep first char constant and add expanded contraction \n",
    "#                                                                    #from 2nd char onwards\n",
    "#         return expanded_contraction\n",
    "        \n",
    "#     expanded_text = contractions_pattern.sub(expand_match, text) # replace matched contraaction pattern with expanded one\n",
    "#     expanded_text = re.sub(\"'\", \"\", expanded_text) #remove the ' from expanded text\n",
    "#     return expanded_text\n",
    "\n",
    "# # function for POS tags \n",
    "# from nltk.corpus import wordnet as wn\n",
    "# def pos_tag_text(text): # convert spacy tags to wordnet tags to use wordnet lemmatizer\n",
    "#     def wn_tags(token_pos):\n",
    "#         if token_pos == 'ADJ':\n",
    "#             return wn.ADJ\n",
    "#         if token_pos == 'VERB':\n",
    "#             return wn.VERB\n",
    "#         if token_pos == 'NOUN':\n",
    "#             return wn.NOUN\n",
    "#         if token_pos == 'ADV':\n",
    "#             return wn.ADV\n",
    "#         else:\n",
    "#             return None\n",
    "#     text = nlp(text)\n",
    "#     tagged_text = [(token.orth_,token.pos_) for token in text]\n",
    "#     tagged_text = [(token[0].lower(),wn_tags(token[1])) for token in tagged_text] # convert tags and words to lowercase\n",
    "#     return tagged_text\n",
    "\n",
    "# # function to lemmatize text based on POS tags \n",
    "# wnl = WordNetLemmatizer()\n",
    "   \n",
    "# def lemmatize_text(text):\n",
    "    \n",
    "#     pos_tagged_text = pos_tag_text(text)\n",
    "#     lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag # return lemmatized word if pos tag present\n",
    "#                          else word                               # just return word if pos tag is \"None\"\n",
    "#                          for word, pos_tag in pos_tagged_text]\n",
    "#     lemmatized_text = ' '.join(lemmatized_tokens) #join tokens with \" \" between them\n",
    "#     return lemmatized_text\n",
    "\n",
    "# # function to remove special character\n",
    "# def remove_special_characters(text):\n",
    "#     tokens = tokenize_text(text)\n",
    "#     pattern = re.compile('[{}]'.format(re.escape(string.punctuation))) #punctuation string from re module\n",
    "#     filtered_tokens = [pattern.sub(' ', token) for token in tokens] #replace matching special character with \" \" \n",
    "#     filtered_text = ' '.join(filtered_tokens) #join tokens with \" \" between them\n",
    "#     return filtered_text\n",
    "\n",
    "# # function to remove stopwords\n",
    "# stopword_list = nltk.corpus.stopwords.words('english') #stopword list from nltk\n",
    "# stopword_list = stopword_list + ['br']\n",
    "# def remove_stopwords(text):\n",
    "#     tokens = tokenize_text(text)\n",
    "#     filtered_tokens = [token for token in tokens if token not in stopword_list] # only extract tokens not in stopword list\n",
    "#     filtered_text = ' '.join(filtered_tokens) #join tokens with \" \" between them\n",
    "#     return filtered_text\n",
    "\n",
    "# # function to normalize corpus (list of sentences!!!!)\n",
    "# def normalize_corpus(corpus, lemmatize=True, tokenize=False):\n",
    "#     normalized_corpus = []    \n",
    "#     for text in corpus:\n",
    "#         text = expand_contractions(text, CONTRACTION_MAP)\n",
    "#         if lemmatize:\n",
    "#             text = lemmatize_text(text)\n",
    "#         else:\n",
    "#             text = text.lower()\n",
    "#         text = remove_special_characters(text)\n",
    "#         text = remove_stopwords(text)\n",
    "#         if tokenize:\n",
    "#             text = tokenize_text(text)\n",
    "#             normalized_corpus.append(text)\n",
    "#         else:\n",
    "#             normalized_corpus.append(text)\n",
    "            \n",
    "#     return normalized_corpus\n",
    "\n",
    "# corpus = normalize_corpus(train['Text'])\n",
    "# corpus_test = normalize_corpus(test['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# train['Text'] = [re.sub('[^0-9a-zA-Z ]+', '', doc) for doc in corpus]\n",
    "# test['Text'] = [re.sub('[^0-9a-zA-Z ]+', '', doc) for doc in corpus_test]\n",
    "# train.to_csv('train_processed2.csv',index=False,header =True)\n",
    "# test.to_csv('test_processed2.csv',index=False,header =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:  (25000, 3)\n",
      "Test data:  (25000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load data and preprocessing\n",
    "\n",
    "train = pd.read_csv('train_processed2.csv')\n",
    "print('Train data: ',train.shape)\n",
    "test = pd.read_csv('test_processed2.csv')\n",
    "print('Test data: ',test.shape)\n",
    "\n",
    "x_test = test['Text']\n",
    "y_test = test['label']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(train['Text'], train['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=3,max_features = 10000, ngram_range=(1, 2))\n",
    "vectorizer.fit(x_train)\n",
    "x_train= vectorizer.transform(x_train)\n",
    "x_val = vectorizer.transform(x_val)\n",
    "x_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86305"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb0 = MultinomialNB()\n",
    "parameters = {'alpha':[0.01,0.1,1,10,100]}\n",
    "mnb = GridSearchCV(mnb0, parameters, scoring='accuracy', cv= 5,n_jobs=-1)\n",
    "mnb.fit(x_train,y_train)\n",
    "mnb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(mnb, open('mnb.sav', 'wb'))\n",
    "#mnb = pickle.load(open('mnb.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88215"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear SVM\n",
    "from sklearn.svm import SVC\n",
    "svm0 = SVC( kernel='linear',random_state =42,probability = True)\n",
    "parameters = {'C':[0.1,1,10]}\n",
    "svm = GridSearchCV(svm0, parameters, scoring='accuracy', cv= 5,n_jobs=-1)\n",
    "svm.fit(x_train,y_train)\n",
    "svm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(svm, open('svm.sav', 'wb'))\n",
    "#svm = pickle.load(open('svm.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.85905"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf0 = RandomForestClassifier(n_estimators=500, random_state =42)\n",
    "parameters = {'max_features':[50,100,500,1000,2000,5000]}\n",
    "rf = GridSearchCV(rf0, parameters, scoring='accuracy', cv= 5,n_jobs=-1)\n",
    "rf.fit(x_train,y_train)\n",
    "rf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(rf, open('rf.sav', 'wb'))\n",
    "#rf = pickle.load(open('rf.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8682"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GBM\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbm0 = GradientBoostingClassifier(random_state =42)\n",
    "parameters = {'n_estimators':[200,500,1000],'max_features':[50,100,500]}\n",
    "gbm = GridSearchCV(gbm0, parameters, scoring='accuracy', cv= 5,n_jobs=-1)\n",
    "gbm.fit(x_train,y_train)\n",
    "gbm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(gbm, open('gbm.sav', 'wb'))\n",
    "#gbm = pickle.load(open('gbm.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1}\n",
      "{'C': 1}\n",
      "{'max_features': 50}\n",
      "{'max_features': 50, 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "print(mnb.best_params_)\n",
    "print(svm.best_params_)\n",
    "print(rf.best_params_)\n",
    "print(gbm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy:  0.88528\n"
     ]
    }
   ],
   "source": [
    "#Stacking\n",
    "\n",
    "mnb_x = mnb.best_estimator_.predict_proba(x_val)\n",
    "svm_x = svm.best_estimator_.predict_proba(x_val)\n",
    "rf_x = rf.best_estimator_.predict_proba(x_val)\n",
    "gbm_x = gbm.best_estimator_.predict_proba(x_val)\n",
    "\n",
    "lvl1_x = pd.concat([pd.DataFrame(mnb_x[:,1]),pd.DataFrame(svm_x[:,1]),pd.DataFrame(rf_x[:,1]),pd.DataFrame(gbm_x[:,1])],axis=1)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lm = LogisticRegression(random_state=42)\n",
    "lm.fit(lvl1_x,y_val)\n",
    "\n",
    "#Evaluation\n",
    "\n",
    "mnb_x_test = mnb.best_estimator_.predict_proba(x_test)\n",
    "svm_x_test = svm.best_estimator_.predict_proba(x_test)\n",
    "rf_x_test = rf.best_estimator_.predict_proba(x_test)\n",
    "gbm_x_test = gbm.best_estimator_.predict_proba(x_test)\n",
    "\n",
    "lvl1_x_test = pd.concat([pd.DataFrame(mnb_x_test[:,1]),pd.DataFrame(svm_x_test[:,1]),pd.DataFrame(rf_x_test[:,1]),pd.DataFrame(gbm_x_test[:,1])],axis=1)\n",
    "\n",
    "pred = lm.predict(lvl1_x_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Average Accuracy: ',accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Playing with Recurrent Neural Networks (RNN)\n",
    "So far, we've only treated text as a simple bag, with reasonable results.  We'll now shift to a more complex representation of language: recurrent neural networks.  To do so, we need to process text at the word or character level, and capture the sequence of a document. \n",
    "\n",
    "Our task here is to build an RNN that 'eats up' sequences of characters in order to predict the next character in a sequence, for every step in the sequence of a document. This is a common (and fun) task, with lots of examples available online. \n",
    "\n",
    "For this task, use existing RNN APIs (don't code everything from scratch) from Keras or PyTorch. \n",
    "\n",
    "**Read up on RNNs and this exercise** below:\n",
    "* http://karpathy.github.io/2015/05/21/rnn-effectiveness/ - start here!\n",
    "* https://github.com/martin-gorner/tensorflow-rnn-shakespeare - video, slides and code going through an example with Shakespeare\n",
    "* http://killianlevacher.github.io/blog/posts/post-2016-03-01/post.html - another nice example based on Trump tweets\n",
    "\n",
    "### Prepare your data\n",
    "\n",
    "Our first step is to prepare our text. **Process your corpora into a format that can be used by an RNN, and walkthough one sequence below**.\n",
    "\n",
    "An **example way to shape your data** for this task is as follows (feel free to play around with different structures):\n",
    "\n",
    "*In this example your corpora starts with the string 'the cat and I'*\n",
    "* RNN input: divide your text into sequences of 10 characters e.g. 'the cat an'\n",
    "* RNN output: the 1 character immediately following RNN input sequences e.g. 'd'. \n",
    "* Note: You may or may not want to divide your text into overlapping strings (e.g. RNN input contains 'the cat an', 'he cat and', 'e cat and ', ...) . How is the model different in each case?\n",
    "* Note: Your 'vocabulary' or `vocab_size` here is the number of unique characters in your text (and therefore the number of classes you want to predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('train_processed2.csv')\n",
    "# test = pd.read_csv('test_processed2.csv')\n",
    "# corpus_test = test['Text'][:10]\n",
    "# x_test = [doc[:64] for doc in corpus_test]\n",
    "\n",
    "# corpus = train.loc[:,'Text'].tolist() + test.loc[10:,'Text'].tolist()\n",
    "\n",
    "# train_list_all =[]\n",
    "# train_label_all = []\n",
    "\n",
    "# for i in range(len(corpus)):\n",
    "#     j = 0\n",
    "#     window = 64\n",
    "#     switch = 1\n",
    "#     train_list = []\n",
    "#     train_label = []\n",
    "\n",
    "#     while switch==1:\n",
    "#         if j+window >= len(corpus[i]): break\n",
    "#         txt = corpus[i][j:j+window]\n",
    "#         train_list.append(txt)\n",
    "#         train_label.append(corpus[i][j+1:j+window+1])\n",
    "#         j = j+window\n",
    "#     train_list_all += train_list\n",
    "#     train_label_all += train_label\n",
    "\n",
    "# trainseq = pd.DataFrame(train_list_all, columns = ['Text'])\n",
    "# trainseq['label'] = train_label_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #trainseq.to_csv('trainseq7.csv',index=False,header=True)\n",
    "# trainseq = pd.read_csv('trainseq7.csv')\n",
    "# trainseq = trainseq.sample(250000,axis=0,random_state=42)\n",
    "# print(trainseq.shape)\n",
    "# trainseq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# x_train, x_val, y_train, y_val = train_test_split(trainseq['Text'], trainseq['label'], test_size=0.1, shuffle=False)\n",
    "\n",
    "# # get the set of all characters\n",
    "# characters = tuple(set([item for sublist in [list(x) for x in x_train] for item in sublist]))\n",
    "\n",
    "# #use enumeration to give the characters integer values\n",
    "# int2char = dict(enumerate(characters))\n",
    "\n",
    "# # create the look up dictionary from characters to the assigned integers\n",
    "# char2int = {char: index for index, char in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(int2char, open('int2char2.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # encode the text, using the character to integer dictionary\n",
    "# x_train_idx = pd.DataFrame([[char2int[char] for char in list(doc)] for doc in x_train])\n",
    "# y_train_idx = pd.DataFrame([[char2int[char] for char in list(doc)] for doc in y_train])\n",
    "# train_idx = pd.concat([x_train_idx,y_train_idx],axis=1)\n",
    "# train_idx.dropna()\n",
    "# x_train_idx = train_idx.iloc[:,:64]\n",
    "# y_train_idx = train_idx.iloc[:,64:]\n",
    "\n",
    "# x_val_idx = pd.DataFrame([[char2int[char] for char in list(doc)] for doc in x_val])\n",
    "# y_val_idx = pd.DataFrame([[char2int[char] for char in list(doc)] for doc in y_val])\n",
    "# val_idx = pd.concat([x_val_idx,y_val_idx],axis=1)\n",
    "# val_idx.dropna()\n",
    "# x_val_idx = val_idx.iloc[:,:64]\n",
    "# y_val_idx = val_idx.iloc[:,64:]\n",
    "\n",
    "# x_test_idx = pd.DataFrame([[char2int[char] for char in list(doc)] for doc in x_test])\n",
    "# x_test_idx = x_test_idx.dropna(axis=0)\n",
    "\n",
    "# x_train_idx.astype(int).to_csv('x_train_idx6.csv',index=False)\n",
    "# x_val_idx.astype(int).to_csv('x_val_idx6.csv',index=False)\n",
    "# x_test_idx.astype(int).to_csv('x_test_idx6.csv',index=False)\n",
    "\n",
    "# y_train_idx.astype(int).to_csv('y_train_idx6.csv',index=False)\n",
    "# y_val_idx.astype(int).to_csv('y_val_idx6.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data as utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('x_train_idx6.csv')\n",
    "# y_train = pd.read_csv('y_train_idx6.csv')\n",
    "# x_train = train.astype(int)\n",
    "# y_train = y_train.astype(int)\n",
    "# print(x_train.shape)\n",
    "\n",
    "# val = pd.read_csv('x_val_idx6.csv')\n",
    "# y_val = pd.read_csv('y_val_idx6.csv')\n",
    "# x_val = val.astype(int)\n",
    "# y_val = y_val.astype(int)\n",
    "# print(x_val.shape)\n",
    "\n",
    "# test = pd.read_csv('x_test_idx6.csv')\n",
    "# x_test = test.astype(int)\n",
    "# print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_xtrain = torch.stack([torch.cuda.FloatTensor(np.eye(37)[i]) for i in x_train.values]) # transform to torch tensors\n",
    "# tensor_ytrain = torch.cuda.LongTensor(np.array(y_train))\n",
    "\n",
    "# tensor_xval = torch.stack([torch.cuda.FloatTensor(np.eye(37)[i]) for i in x_val.values]) # transform to torch tensors\n",
    "# tensor_yval = torch.cuda.LongTensor(np.array(y_val))\n",
    "\n",
    "# trainset = utils.TensorDataset(tensor_xtrain,tensor_ytrain)\n",
    "# valset = utils.TensorDataset(tensor_xval,tensor_yval)\n",
    "\n",
    "# batch_size =50\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(trainset,batch_size=batch_size,shuffle=True)\n",
    "# validation_loader = torch.utils.data.DataLoader(valset,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "# data_loaders = {\"train\": train_loader, \"val\": validation_loader}\n",
    "# data_lengths = {\"train\": x_train.shape[0]/batch_size, \"val\": x_val.shape[0]/batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text\n",
    "\n",
    "Once the model is trained, we can use it to generate completely new text in the style of your training data.  **Train a model using your original choice of corpus below, and generate some sample sentences.** Don't worry too much about your loss / accuracy during training, but instead check on the text your model is generating. Your generated text should be somewhat coherent, i.e. similar to your training text in structure, and not excessively mispelled.\n",
    "\n",
    "An **example model architecture** is as follows (feel free to play around with different structures):\n",
    "* Embedding (for each character in your vocab) of dimension 64\n",
    "* Dropout of 20% for the embedding input to the RNN\n",
    "* 2 LSTM layers, each of dimension 512 (play around with the number and dimension of hidden layers)\n",
    "* Dropout of 50% for each LSTM layer\n",
    "* Dense softmax layer of same dimension as your vocab size (e.g. if your vocab size is 100, this layer is the probabilty that your output is one of 100 possible characters)\n",
    "    \n",
    "**You should understand what each of the above elements are and how they work at a high level by the end of this week's exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Char-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#pickle.dump(data_loaders, open('dataload5b.pickle', 'wb'))\n",
    "data_loaders = pickle.load(open('dataload5b.pickle', 'rb'))\n",
    "\n",
    "#pickle.dump(data_lengths, open('datalen5b.pickle', 'wb'))\n",
    "data_lengths = pickle.load(open('datalen5b.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(37,64)\n",
    "        self.LSTM1 = nn.LSTM(input_size = 64, hidden_size = 512,num_layers=2,dropout=0.5,batch_first=True)\n",
    "        self.fc2 =  nn.Linear(512,37)\n",
    "        \n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.drop(self.fc1(x))\n",
    "        x,hidden = self.LSTM1(x.view(-1,64,64))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "net = net.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('lstm64-3.pt'))\n",
    "loss_df = pd.read_csv('loss64-3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "----------\n",
      "train Loss: 89.9051\n",
      "val Loss: 86.6887\n",
      "\n",
      "Epoch 2/20\n",
      "----------\n",
      "train Loss: 89.9038\n",
      "val Loss: 86.6815\n",
      "\n",
      "Epoch 3/20\n",
      "----------\n",
      "train Loss: 89.9073\n",
      "val Loss: 86.6846\n",
      "\n",
      "Epoch 4/20\n",
      "----------\n",
      "train Loss: 89.8961\n",
      "val Loss: 86.6835\n",
      "\n",
      "Epoch 5/20\n",
      "----------\n",
      "train Loss: 89.8991\n",
      "val Loss: 86.6804\n",
      "\n",
      "Epoch 6/20\n",
      "----------\n",
      "train Loss: 89.8921\n",
      "val Loss: 86.6803\n",
      "\n",
      "Epoch 7/20\n",
      "----------\n",
      "train Loss: 89.8976\n",
      "val Loss: 86.6843\n",
      "\n",
      "Epoch 8/20\n",
      "----------\n",
      "train Loss: 89.8867\n",
      "val Loss: 86.6808\n",
      "\n",
      "Epoch 9/20\n",
      "----------\n",
      "train Loss: 89.8876\n",
      "val Loss: 86.6776\n",
      "\n",
      "Epoch 10/20\n",
      "----------\n",
      "train Loss: 89.8903\n",
      "val Loss: 86.6769\n",
      "\n",
      "Epoch 11/20\n",
      "----------\n",
      "train Loss: 89.8773\n",
      "val Loss: 86.6775\n",
      "\n",
      "Epoch 12/20\n",
      "----------\n",
      "train Loss: 89.8877\n",
      "val Loss: 86.6730\n",
      "\n",
      "Epoch 13/20\n",
      "----------\n",
      "train Loss: 89.8704\n",
      "val Loss: 86.6747\n",
      "\n",
      "Epoch 14/20\n",
      "----------\n",
      "train Loss: 89.8739\n",
      "val Loss: 86.6752\n",
      "\n",
      "Epoch 15/20\n",
      "----------\n",
      "train Loss: 89.8791\n",
      "val Loss: 86.6715\n",
      "\n",
      "Epoch 16/20\n",
      "----------\n",
      "train Loss: 89.8724\n",
      "val Loss: 86.6740\n",
      "\n",
      "Epoch 17/20\n",
      "----------\n",
      "train Loss: 89.8738\n",
      "val Loss: 86.6746\n",
      "\n",
      "Epoch 18/20\n",
      "----------\n",
      "train Loss: 89.8764\n",
      "val Loss: 86.6721\n",
      "\n",
      "Epoch 19/20\n",
      "----------\n",
      "train Loss: 89.8732\n",
      "val Loss: 86.6714\n",
      "\n",
      "Epoch 20/20\n",
      "----------\n",
      "train Loss: 89.8668\n",
      "val Loss: 86.6745\n",
      "\n",
      "\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "n_epochs=20\n",
    "patience = 20\n",
    "\n",
    "early_stop = [np.Inf]*patience\n",
    "train_loss,val_loss,learn = [], [], []\n",
    "np.random.seed(1)\n",
    "\n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "    print('Epoch {}/{}'.format(epoch+1, n_epochs))\n",
    "    print('-' * 10)\n",
    "  \n",
    "    # learning rate schedule\n",
    "    elp = 5\n",
    "    decay = 0.5\n",
    "    #rate = 0.01\n",
    "    rate = loss_df['Learning Rate'].tolist()[-1]*0.5   \n",
    "    \n",
    "    if epoch == 0: lrate = rate\n",
    "    elif epoch >0 and epoch%elp ==0 : lrate = rate * (decay**(epoch//elp))\n",
    "    elif epoch >0 and epoch%elp !=0 : lrate = rate * (decay**(epoch//elp))       \n",
    "    learn.append(lrate)        \n",
    "        \n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            #optimizer = optim.SGD(net.parameters(), lr=lrate,momentum=0.9,nesterov=True,weight_decay=5e-4)\n",
    "            optimizer = optim.Adam(net.parameters(), lr=lrate, betas=(0.9, 0.999), eps=1e-08, weight_decay=5e-3)\n",
    "            net.train(True)  # Set model to training mode\n",
    "        else:\n",
    "            net.train(False)    \n",
    "    \n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for data in data_loaders[phase]:#iteration loop for each minibatch\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            \n",
    "            # zero the parameter gradients to accumulate again using loss.backward() in every iteration\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize              \n",
    "            outputs = net(inputs) # forward\n",
    "            loss = 0\n",
    "            for i in range(64):\n",
    "                loss += criterion(outputs[:,i,:], labels[:,i]) #get loss \n",
    "            \n",
    "            if phase == 'train':\n",
    "                loss.backward() #backpropagation\n",
    "                optimizer.step()  #update parameters\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()        \n",
    "        \n",
    "        epoch_loss = running_loss / data_lengths[phase]\n",
    "        print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "        if phase == 'train': train_loss.append(epoch_loss)\n",
    "        if phase == 'val': val_loss.append(epoch_loss)\n",
    "    \n",
    "    print()\n",
    "          \n",
    "    if phase == 'val': #early stopping block        \n",
    "        if len(early_stop) == patience: del early_stop[0]        \n",
    "        early_stop.append(running_loss)\n",
    "        if min(early_stop) == early_stop[0] : break      \n",
    "\n",
    "print('\\nFinished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "loss_df1 = pd.read_csv('loss64-1.csv')\n",
    "loss_df2 = pd.read_csv('loss64-2.csv')\n",
    "loss_df3 = pd.read_csv('loss64-3.csv')\n",
    "loss_df4 = pd.read_csv('loss64-4.csv')\n",
    "loss_df_all = pd.concat([loss_df1,loss_df2,loss_df3,loss_df4],axis=0)\n",
    "train_loss = loss_df_all['Train Loss']\n",
    "val_loss = loss_df_all['Val Loss']\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "plt.plot(list(range(1,len(train_loss)+1)),train_loss,label='Train Loss')\n",
    "plt.plot(list(range(1,len(val_loss)+1)),val_loss,label='Val Loss')\n",
    "ax.set_title('Train/Val Loss Vs No. of Epochs')\n",
    "x_axis = plt.xlabel('No. of Epochs')\n",
    "y_axis = plt.ylabel('Train/Val Loss')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of Text Generation at Character-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "int2char = pickle.load(open('int2char2.pickle', 'rb'))\n",
    "\n",
    "test = pd.read_csv('x_test_idx6.csv')\n",
    "x_test = test.astype(int)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#003366\"><b> IMDB movie reviews data used in part 2 were used as training data for the character generation model. The first 10 reviews of the test set was set aside as seeds to generate text. The rest of the reviews were cut into 64 character sequence and 250,000 sequences were sampled for training (memory constraint). The vocabulary consist of 37 characters (a-z, 0-9 and ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy sampling Vs Multinomial sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed Text from Test data: \n",
      "watch grand champion way least twice enjoy movie story character\n",
      "\n",
      "Greedy Sampling (max softmax probability): \n",
      "watch grand champion way least twice enjoy movie story character see movie see movie see movie see movie see movie see movie see movie see movie see movie see movie\n",
      "\n",
      "Multinomial Sampling and Temperature scaling: \n",
      "watch grand champion way least twice enjoy movie story character film bad movie ever want studio masterpiece happen movie film probably movie especially actually mi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Temperature Scaling and multinormial sampling\n",
    "\n",
    "n = 2\n",
    "T = 0.8\n",
    "genlen = 100\n",
    "\n",
    "print('Seed Text from Test data: ')\n",
    "print(''.join([int2char[char] for char in  x_test.iloc[n,:].tolist()]))\n",
    "print()\n",
    "\n",
    "inputs = torch.stack([torch.cuda.FloatTensor(np.eye(37)[i]) for i in x_test.iloc[n,:].tolist()])\n",
    "inputs = inputs.reshape(1,64,37)\n",
    "\n",
    "generate = inputs.clone()\n",
    "with torch.no_grad():\n",
    "    for i in range (genlen):\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 2)\n",
    "        generate = torch.cat([generate,torch.cuda.FloatTensor(np.eye(37)[predicted][:,63]).reshape(1,1,37)],1)\n",
    "        inputs = torch.cat([inputs[:,1:,:],torch.cuda.FloatTensor(np.eye(37)[predicted][:,63]).reshape(1,1,37)],1)\n",
    "    \n",
    "gentext = [int(np.where(char==1)[0]) for char in generate[0]]\n",
    "\n",
    "print('Greedy Sampling (max softmax probability): ')\n",
    "print(''.join([int2char[char] for char in gentext]))\n",
    "print()\n",
    "\n",
    "inputs = torch.stack([torch.cuda.FloatTensor(np.eye(37)[i]) for i in x_test.iloc[n,:].tolist()])\n",
    "inputs = inputs.reshape(1,64,37)\n",
    "\n",
    "generate = inputs.clone()\n",
    "with torch.no_grad():\n",
    "    for i in range (genlen):\n",
    "        outputs= net(inputs)\n",
    "        nxt = torch.distributions.multinomial.Multinomial(total_count=1, probs= F.softmax(outputs.data[0,63,:]/T,0)).sample()\n",
    "        generate = torch.cat([generate,torch.cuda.FloatTensor(nxt).reshape(1,1,37)],1)\n",
    "        inputs = torch.cat([inputs[:,1:,:],torch.cuda.FloatTensor(nxt).reshape(1,1,37)],1)\n",
    "    \n",
    "gentext = [int(np.where(char==1)[0]) for char in generate[0]]\n",
    "\n",
    "print('Multinomial Sampling and Temperature scaling: ')\n",
    "print(''.join([int2char[char] for char in gentext]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature Parameter and Linguistic Variety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature = 0.1: \n",
      "australian currently live japan saw movie tv impressed accuracy story start see movie see movie see movie see movie see movie see movie see movie see story see movi\n",
      "\n",
      "Temperatue = 0.8: \n",
      "australian currently live japan saw movie tv impressed accuracy director make idea begin young show boy action also last review time everybody love deserve take str\n",
      "\n",
      "Temperatue = 1.0: \n",
      "australian currently live japan saw movie tv impressed accuracy bring end creation dolic film book beat one cure nazis china complex ill relavious hell pitiful occh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Temperature Parameter\n",
    "\n",
    "n = 3\n",
    "genlen = 100\n",
    "\n",
    "inputs = torch.stack([torch.cuda.FloatTensor(np.eye(37)[i]) for i in x_test.iloc[n,:].tolist()])\n",
    "inputs = inputs.reshape(1,64,37)\n",
    "\n",
    "generate = inputs.clone()\n",
    "with torch.no_grad():\n",
    "    for i in range (genlen):\n",
    "        outputs= net(inputs)\n",
    "        nxt = torch.distributions.multinomial.Multinomial(total_count=1, probs= F.softmax(outputs.data[0,63,:]/0.1,0)).sample()\n",
    "        generate = torch.cat([generate,torch.cuda.FloatTensor(nxt).reshape(1,1,37)],1)\n",
    "        inputs = torch.cat([inputs[:,1:,:],torch.cuda.FloatTensor(nxt).reshape(1,1,37)],1)\n",
    "    \n",
    "gentext = [int(np.where(char==1)[0]) for char in generate[0]]\n",
    "\n",
    "print('Temperature = 0.1: ')\n",
    "print(''.join([int2char[char] for char in gentext]))\n",
    "print()\n",
    "\n",
    "inputs = torch.stack([torch.cuda.FloatTensor(np.eye(37)[i]) for i in x_test.iloc[n,:].tolist()])\n",
    "inputs = inputs.reshape(1,64,37)\n",
    "\n",
    "generate = inputs.clone()\n",
    "with torch.no_grad():\n",
    "    for i in range (genlen):\n",
    "        outputs= net(inputs)\n",
    "        nxt = torch.distributions.multinomial.Multinomial(total_count=1, probs= F.softmax(outputs.data[0,63,:]/0.8,0)).sample()\n",
    "        generate = torch.cat([generate,torch.cuda.FloatTensor(nxt).reshape(1,1,37)],1)\n",
    "        inputs = torch.cat([inputs[:,1:,:],torch.cuda.FloatTensor(nxt).reshape(1,1,37)],1)\n",
    "    \n",
    "gentext = [int(np.where(char==1)[0]) for char in generate[0]]\n",
    "\n",
    "print('Temperatue = 0.8: ')\n",
    "print(''.join([int2char[char] for char in gentext]))\n",
    "print()\n",
    "\n",
    "inputs = torch.stack([torch.cuda.FloatTensor(np.eye(37)[i]) for i in x_test.iloc[n,:].tolist()])\n",
    "inputs = inputs.reshape(1,64,37)\n",
    "\n",
    "generate = inputs.clone()\n",
    "with torch.no_grad():\n",
    "    for i in range (genlen):\n",
    "        outputs= net(inputs)\n",
    "        nxt = torch.distributions.multinomial.Multinomial(total_count=1, probs= F.softmax(outputs.data[0,63,:]/1.0,0)).sample()\n",
    "        generate = torch.cat([generate,torch.cuda.FloatTensor(nxt).reshape(1,1,37)],1)\n",
    "        inputs = torch.cat([inputs[:,1:,:],torch.cuda.FloatTensor(nxt).reshape(1,1,37)],1)\n",
    "    \n",
    "gentext = [int(np.where(char==1)[0]) for char in generate[0]]\n",
    "\n",
    "print('Temperatue = 1.0: ')\n",
    "print(''.join([int2char[char] for char in gentext]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#003366\"><b> If the values of the scaled output is small (e.g. 1 or 1.1 , temperature parameter = 1), the probability difference between different output values are small. Very noisy sampling is observed with different characters chosen everytime. If the values of the scaled output is large (e.g. 10 or 11 , temperature parameter = 0.1), the probability difference between different output values are large and the same character is chosen everytime A simple realization of this property is shown below.\n",
    "    \n",
    "<img src=\"./scaling.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "movie right away see coen movie exception raising arizona notice law run cover surprised moment hollywood quite girl could life scimit great film since chich want c\n",
      "\n",
      "Epoch 5:\n",
      "movie right away see coen movie exception raising arizona notice great korrow pretty really time film drive past land knower powellod really snook posil around awis\n",
      "\n",
      "Epoch 80\n",
      "movie right away see coen movie exception raising arizona notice extremely like christ street vidio scene moment murder scene movie thing consider director command \n"
     ]
    }
   ],
   "source": [
    "#Train progression\n",
    "\n",
    "n = 0\n",
    "T = 0.8\n",
    "genlen = 100\n",
    "\n",
    "net1 = net\n",
    "net5 = net\n",
    "net1.load_state_dict(torch.load('lstm64-00.pt'))\n",
    "net5.load_state_dict(torch.load('lstm64-0.pt'))\n",
    "\n",
    "print('Epoch 1:')\n",
    "\n",
    "inputs = torch.stack([torch.cuda.FloatTensor(np.eye(37)[i]) for i in x_test.iloc[n,:].tolist()])\n",
    "inputs = inputs.reshape(1,64,37)\n",
    "\n",
    "generate = inputs.clone()\n",
    "with torch.no_grad():\n",
    "    for i in range (genlen):\n",
    "        outputs = net1(inputs)\n",
    "        nxt = torch.distributions.multinomial.Multinomial(total_count=1, probs= F.softmax(outputs.data[0,63,:]/T,0)).sample()\n",
    "        generate = torch.cat([generate,torch.cuda.FloatTensor(nxt).reshape(1,1,37)],1)\n",
    "        inputs = torch.cat([inputs[:,1:,:],torch.cuda.FloatTensor(nxt).reshape(1,1,37)],1)\n",
    "    \n",
    "gentext = [int(np.where(char==1)[0]) for char in generate[0]]\n",
    "\n",
    "print(''.join([int2char[char] for char in gentext]))\n",
    "print()\n",
    "\n",
    "print('Epoch 5:')\n",
    "\n",
    "inputs = torch.stack([torch.cuda.FloatTensor(np.eye(37)[i]) for i in x_test.iloc[n,:].tolist()])\n",
    "inputs = inputs.reshape(1,64,37)\n",
    "\n",
    "generate = inputs.clone()\n",
    "with torch.no_grad():\n",
    "    for i in range (genlen):\n",
    "        outputs = net5(inputs)\n",
    "        nxt = torch.distributions.multinomial.Multinomial(total_count=1, probs= F.softmax(outputs.data[0,63,:]/T,0)).sample()\n",
    "        generate = torch.cat([generate,torch.cuda.FloatTensor(nxt).reshape(1,1,37)],1)\n",
    "        inputs = torch.cat([inputs[:,1:,:],torch.cuda.FloatTensor(nxt).reshape(1,1,37)],1)\n",
    "    \n",
    "gentext = [int(np.where(char==1)[0]) for char in generate[0]]\n",
    "\n",
    "print(''.join([int2char[char] for char in gentext]))\n",
    "print()\n",
    "\n",
    "print('Epoch 80')\n",
    "\n",
    "inputs = torch.stack([torch.cuda.FloatTensor(np.eye(37)[i]) for i in x_test.iloc[n,:].tolist()])\n",
    "inputs = inputs.reshape(1,64,37)\n",
    "\n",
    "generate = inputs.clone()\n",
    "with torch.no_grad():\n",
    "    for i in range (genlen):\n",
    "        outputs = net(inputs)\n",
    "        nxt = torch.distributions.multinomial.Multinomial(total_count=1, probs= F.softmax(outputs.data[0,63,:]/T,0)).sample()\n",
    "        generate = torch.cat([generate,torch.cuda.FloatTensor(nxt).reshape(1,1,37)],1)\n",
    "        inputs = torch.cat([inputs[:,1:,:],torch.cuda.FloatTensor(nxt).reshape(1,1,37)],1)\n",
    "    \n",
    "gentext = [int(np.where(char==1)[0]) for char in generate[0]]\n",
    "\n",
    "print(''.join([int2char[char] for char in gentext]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#003366\"><b> 1. 64 character sequence: Longer sequences to take advantage of LSTM long range memory capabilities. Karpathy's char-rnn has 50 character sequences. Other character RNNs have ~100 character sequences. Final model was obtained after training for 80 epochs <br/>\n",
    "       \n",
    "<span style=\"color:#003366\"><b> 2. Temperature scaling and multinomial sampling: Avoid generating repeated character sequences. Instead of taking the character with the max softmax probability, sample character from multinomial distribution with softmax probilities. Tempearature scaling of the RNN output before softmax activation can increase linguistic variety of generated text at a higher value for the temperature parameter\n",
    "\n",
    "<span style=\"color:red\"><b>Additional Note    \n",
    "<span style=\"color:#003366\"><b>Stateful LSTM: Take character sequences as originating from a long string. Feed hidden state of each batch to the next batch ( at batch size 1) instead of initializing to zero, assumption that the batches are connected end-to-end instead of being indepedent. Useful for long novel type text data for faster convergence. Not used here as randomly sampled subsequences are not related end-to-end <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalizing the exercise\n",
    "How do you think you can apply what you learned in the above exercise to other problems involving text? For example, how would you tackle the previous IMDB sentiment classification task using an RNN architecture? **Discuss below.**\n",
    "\n",
    "(*Bonus*: create an RNN model for the IMDB classification task and discuss your results. How does the performance compare to your bag of words model?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "vocab_size = 10000\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_processed2.csv')\n",
    "pos_corpus = train.loc[0:12499,'Text']\n",
    "neg_corpus = train.loc[12500:,'Text']\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = vocab_size-1, ngram_range=(1, 1))\n",
    "vectorizer.fit(train['Text'])\n",
    "names = vectorizer.get_feature_names()\n",
    "word2int = dict(tuple(zip(names,range(1,len(names)))))\n",
    "\n",
    "corpus_words= [doc.split(' ') for doc in pos_corpus]\n",
    "pos_corpus_all = [[word for word in text if word in word2int.keys()] for text in corpus_words]\n",
    "\n",
    "for i in range(len(pos_corpus_all)):\n",
    "    for j in range(len(pos_corpus_all[i])):\n",
    "        pos_corpus_all[i][j] = word2int[pos_corpus_all[i][j]]\n",
    "        \n",
    "corpus_words= [doc.split(' ') for doc in neg_corpus]\n",
    "neg_corpus_all  = [[word for word in text if word in word2int.keys()] for text in corpus_words]\n",
    "\n",
    "for i in range(len(neg_corpus_all)):\n",
    "    for j in range(len(neg_corpus_all[i])):\n",
    "        neg_corpus_all[i][j] = word2int[neg_corpus_all[i][j]]\n",
    "        \n",
    "x_train = pos_corpus_all+neg_corpus_all\n",
    "y_train = train['label']\n",
    "\n",
    "# import pickle\n",
    "# pickle.dump(x_train, open('x_train.pickle', 'wb'))\n",
    "# pickle.dump(y_train, open('y_train.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_processed2.csv')\n",
    "pos_corpus = test.loc[0:12499,'Text']\n",
    "neg_corpus = test.loc[12500:,'Text']\n",
    "\n",
    "corpus_words= [doc.split(' ') for doc in pos_corpus]\n",
    "pos_corpus_all = [[word for word in text if word in word2int.keys()] for text in corpus_words]\n",
    "\n",
    "for i in range(len(pos_corpus_all)):\n",
    "    for j in range(len(pos_corpus_all[i])):\n",
    "        pos_corpus_all[i][j] = word2int[pos_corpus_all[i][j]]\n",
    "        \n",
    "corpus_words= [doc.split(' ') for doc in neg_corpus]\n",
    "neg_corpus_all = [[word for word in text if word in word2int.keys()] for text in corpus_words]\n",
    "\n",
    "for i in range(len(neg_corpus_all)):\n",
    "    for j in range(len(neg_corpus_all[i])):\n",
    "        neg_corpus_all[i][j] = word2int[neg_corpus_all[i][j]]\n",
    "        \n",
    "x_test = pos_corpus_all+neg_corpus_all\n",
    "y_test = test['label']\n",
    "\n",
    "# import pickle\n",
    "# pickle.dump(x_test, open('x_test.pickle', 'wb'))\n",
    "# pickle.dump(y_test, open('y_test.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 100)\n",
      "(5000, 100)\n",
      "(25000, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "from keras.regularizers import l2\n",
    "\n",
    "reg_param = 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 300, input_length=maxlen))\n",
    "#model.add(LSTM(256,kernel_initializer='he_normal',return_sequences=True, kernel_regularizer = l2(reg_param),recurrent_regularizer = l2(reg_param),dropout = 0.5, recurrent_dropout = 0.5))\n",
    "model.add(LSTM(256,kernel_initializer='he_normal', kernel_regularizer = l2(reg_param),recurrent_regularizer = l2(reg_param),dropout = 0.5, recurrent_dropout = 0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "adam = Adam(beta_1=0.9, beta_2=0.999, decay=0)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          900000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               570368    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,470,625\n",
      "Trainable params: 1,470,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "20000/20000 [==============================] - 62s 3ms/step - loss: 322.2807 - acc: 0.6400 - val_loss: 1.4209 - val_acc: 0.5826\n",
      "Epoch 2/30\n",
      "20000/20000 [==============================] - 61s 3ms/step - loss: 0.6549 - acc: 0.7647 - val_loss: 0.5645 - val_acc: 0.8136\n",
      "Epoch 3/30\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.4195 - acc: 0.8516 - val_loss: 0.4219 - val_acc: 0.8424\n",
      "Epoch 4/30\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.3739 - acc: 0.8670 - val_loss: 0.3965 - val_acc: 0.8462\n",
      "Epoch 5/30\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.3351 - acc: 0.8821 - val_loss: 0.3674 - val_acc: 0.8574\n",
      "Epoch 6/30\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.3229 - acc: 0.8846 - val_loss: 0.3737 - val_acc: 0.8524\n",
      "Epoch 7/30\n",
      "20000/20000 [==============================] - 62s 3ms/step - loss: 0.3088 - acc: 0.8910 - val_loss: 0.3620 - val_acc: 0.8572\n",
      "Epoch 8/30\n",
      "20000/20000 [==============================] - 61s 3ms/step - loss: 0.3056 - acc: 0.8922 - val_loss: 0.3591 - val_acc: 0.8568\n",
      "Epoch 9/30\n",
      "20000/20000 [==============================] - 59s 3ms/step - loss: 0.2977 - acc: 0.8950 - val_loss: 0.3535 - val_acc: 0.8624\n",
      "Epoch 10/30\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.2950 - acc: 0.8948 - val_loss: 0.3527 - val_acc: 0.8626\n",
      "Epoch 11/30\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.2899 - acc: 0.8963 - val_loss: 0.3500 - val_acc: 0.8626\n",
      "Epoch 12/30\n",
      "20000/20000 [==============================] - 62s 3ms/step - loss: 0.2902 - acc: 0.8959 - val_loss: 0.3467 - val_acc: 0.8638\n",
      "Epoch 13/30\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.2877 - acc: 0.8982 - val_loss: 0.3452 - val_acc: 0.8636\n",
      "Epoch 14/30\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.2861 - acc: 0.8973 - val_loss: 0.3458 - val_acc: 0.8654\n",
      "Epoch 15/30\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.2855 - acc: 0.8990 - val_loss: 0.3443 - val_acc: 0.8642\n",
      "Epoch 16/30\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.2840 - acc: 0.8984 - val_loss: 0.3441 - val_acc: 0.8642\n",
      "Epoch 17/30\n",
      "20000/20000 [==============================] - 62s 3ms/step - loss: 0.2850 - acc: 0.8986 - val_loss: 0.3446 - val_acc: 0.8642\n",
      "Epoch 18/30\n",
      "20000/20000 [==============================] - 61s 3ms/step - loss: 0.2856 - acc: 0.8979 - val_loss: 0.3444 - val_acc: 0.8630\n",
      "Epoch 19/30\n",
      "20000/20000 [==============================] - 61s 3ms/step - loss: 0.2825 - acc: 0.8986 - val_loss: 0.3443 - val_acc: 0.8634\n",
      "Epoch 20/30\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.2825 - acc: 0.8994 - val_loss: 0.3441 - val_acc: 0.8636\n",
      "Epoch 21/30\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.2859 - acc: 0.8985 - val_loss: 0.3442 - val_acc: 0.8632\n",
      "Epoch 22/30\n",
      "20000/20000 [==============================] - 62s 3ms/step - loss: 0.2833 - acc: 0.8986 - val_loss: 0.3441 - val_acc: 0.8646\n",
      "Epoch 23/30\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.2821 - acc: 0.8996 - val_loss: 0.3440 - val_acc: 0.8642\n",
      "Epoch 24/30\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.2841 - acc: 0.8987 - val_loss: 0.3439 - val_acc: 0.8642\n",
      "Epoch 25/30\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.2837 - acc: 0.8989 - val_loss: 0.3439 - val_acc: 0.8642\n",
      "Epoch 26/30\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.2836 - acc: 0.8994 - val_loss: 0.3439 - val_acc: 0.8640\n",
      "Epoch 27/30\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.2823 - acc: 0.8987 - val_loss: 0.3439 - val_acc: 0.8642\n",
      "Epoch 28/30\n",
      "20000/20000 [==============================] - 62s 3ms/step - loss: 0.2833 - acc: 0.8989 - val_loss: 0.3439 - val_acc: 0.8642\n",
      "Epoch 29/30\n",
      "20000/20000 [==============================] - 61s 3ms/step - loss: 0.2841 - acc: 0.8990 - val_loss: 0.3439 - val_acc: 0.8642\n",
      "Epoch 30/30\n",
      "20000/20000 [==============================] - 60s 3ms/step - loss: 0.2838 - acc: 0.8991 - val_loss: 0.3439 - val_acc: 0.8642\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def lr_sch(epoch):\n",
    "    elp = 2\n",
    "    decay = 0.5\n",
    "    rate = 0.001\n",
    "    \n",
    "    if epoch == 0: lrate = rate\n",
    "    elif epoch >0 and epoch%elp ==0 : lrate = rate * (decay**(epoch//elp))\n",
    "    elif epoch >0 and epoch%elp !=0 : lrate = rate * (decay**(epoch//elp))  \n",
    "    return lrate\n",
    "\n",
    "lrsch = LearningRateScheduler(lr_sch)\n",
    "\n",
    "\n",
    "#early stopping\n",
    "early_stop = EarlyStopping(monitor= 'val_loss', min_delta = 0.0000, patience=5, mode='min')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#model_fit\n",
    "modelfit = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=64,\n",
    "        epochs=30,\n",
    "        verbose=1,\n",
    "        validation_data = (x_val,y_val),\n",
    "        callbacks=[lrsch,early_stop] \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "trainloss = pd.DataFrame(modelfit.history['loss'])\n",
    "trainacc = pd.DataFrame(modelfit.history['acc'])\n",
    "valloss = pd.DataFrame(modelfit.history['val_loss'])\n",
    "valacc = pd.DataFrame(modelfit.history['val_acc'])\n",
    "\n",
    "summary = pd.concat([trainloss,trainacc,valloss,valacc],axis=1)\n",
    "summary = summary.iloc[1:,:]\n",
    "summary.columns = [\"train loss\",\"train acc\",\"val loss\",\"val acc\"]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "f, ax = plt.subplots(1,2,figsize=(15, 7))\n",
    "ax[0].plot(summary.index+1,summary['train acc'],label='Train Accuracy')\n",
    "ax[0].plot(summary.index+1,summary['val acc'],label='Val Accuracy')\n",
    "x_axis = ax[0].set_xlabel('No. of Epochs')\n",
    "y_axis = ax[0].set_ylabel('Accuracy')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(summary.index+1,summary['train loss'],label='Train Loss')\n",
    "ax[1].plot(summary.index+1,summary['val loss'],label='Val Loss')\n",
    "x_axis = ax[1].set_xlabel('No. of Epochs')\n",
    "y_axis = ax[1].set_ylabel('Loss')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 51s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3422108015727997, 0.86184]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model1.save('mlp/mlp_best.h5')\n",
    "\n",
    "evaluate = model.evaluate(x_test,y_test)\n",
    "evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#003366\"><b> Reviews were pre-processed and tokenized and the vocabulary size was set to 3000. Words outside the top 3000 words (in terms of tfidf scores) are removed from the reviews. The words were then converted to integers correpsonding to each of the top 3000 wods. 0s were resereved for padding. Sequence length for each review were standardized to 200 words. Reviews longer than 200 words were truncated to 200 words while reviews less than 200 words were padded with zeros in front of the reviews to 200 words. \n",
    "    \n",
    "<span style=\"color:#003366\"><b> A single-layer LSTM model was trained on the prepared data. Severe overfitting was observed where the train loss/accuracy deviates significantly from the val loss/accuracy. It appears it is quite hard to tune the regularization parameters for the LSTM model and tuning the vocab size and the max sequence length is much easier to adjust the complexity of the model. Reducing the vocab size takes out unimportant words that add noise to the data. \n",
    "    \n",
    "<span style=\"color:#003366\"><b> Despite severe overfitting in a very simple single-layer LSTM model, an accuracy of 86.2% was achieved that is comparable to most of the single bag of words models. There are still plenty of room for improving the model by better tuning model regularization, using bidirectional LSTM to learn the surrounding words(in front and behind) and using pre-trained word embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: RNNs from scratch\n",
    "Now that you understand how to use RNNs, it's time to build a basic one from scratch.  You won't understand how they work until you get stuck in the weeds! \n",
    "\n",
    "### Generate text (Version 2)\n",
    "Your task is now to **build the forward pass of a simple RNN, without using any existing RNN APIs**. You can use PyTorch or Tensorflow (Keras is too high level for this exercise), both of which will automatically handle backpropagation for you.  If you use Tensorflow, please research and use Eager execution - it replaces Tensorflow's default graph / session framework, which is very difficult to learn and debug.\n",
    "\n",
    "Similar to last week's exercise, create a class for your network (write forward and loss steps, allowing PyTorch or Tensorflow to handle backpropagation for you).  Consider appropriate sizes for your input, hidden and output layers - your __init__ method should take in the params `hidden_size`, `vocab_size`, and `embedding_size` (if you use embeddings). Using these variables, you should initialise three weight layers `input_layer`, `hidden_layer`, and `output_layer`.  In an RNN, you will also have to deal with another item - the `hidden_state`. (Note: your RNN structure may vary slightly from this depending on your learning materials, but the key part is always `hidden_state`)\n",
    "\n",
    "You should **train your RNN on the same data and task as in Chapter 3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data as utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data_loaders = pickle.load(open('dataload10.pickle', 'rb'))\n",
    "data_lengths = pickle.load(open('datalen10.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rnn import RNN\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(37,64)\n",
    "        self.rnn1 = RNN(64,512) # Implementation from scratch\n",
    "        self.fc2 =  nn.Linear(512,37)\n",
    "        \n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.drop(self.fc1(x))\n",
    "        x = self.rnn1.forward(x.view(-1,10,64))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "net = net.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.load_state_dict(torch.load('lstm10-2.pt'))\n",
    "# loss_df = pd.read_csv('loss10-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs= 15\n",
    "patience = 15\n",
    "\n",
    "early_stop = [np.Inf]*patience\n",
    "train_loss,val_loss = [], []\n",
    "learn = []\n",
    "np.random.seed(1)\n",
    "\n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "    print('Epoch {}/{}'.format(epoch+1, n_epochs))\n",
    "    print('-' * 10)\n",
    "  \n",
    "    # learning rate schedule\n",
    "    elp = 3\n",
    "    decay = 0.5\n",
    "    rate = 0.01\n",
    "    #rate = loss_df['Learning Rate'][0]*0.5   \n",
    "        \n",
    "    if epoch == 0: lrate = rate\n",
    "    elif epoch >0 and epoch%elp ==0 : lrate = rate * (decay**(epoch//elp))\n",
    "    elif epoch >0 and epoch%elp !=0 : lrate = rate * (decay**(epoch//elp))   \n",
    "    learn.append(lrate)\n",
    "\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            #optimizer = optim.SGD(net.parameters(), lr=lrate,momentum=0.9,nesterov=True,weight_decay=5e-4)\n",
    "            optimizer = optim.Adam(net.parameters(), lr=lrate, betas=(0.9, 0.999), eps=1e-08, weight_decay=5e-3)\n",
    "            net.train(True)  # Set model to training mode\n",
    "        else:\n",
    "            net.train(False)    \n",
    "    \n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for data in data_loaders[phase]:#iteration loop for each minibatch\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            \n",
    "            # zero the parameter gradients to accumulate again using loss.backward() in every iteration\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs) # forward\n",
    "            loss = 0\n",
    "            for i in range(10):\n",
    "                loss += criterion(outputs[:,i,:], labels[:,i]) #get loss\n",
    "            \n",
    "            if phase == 'train':\n",
    "                loss.backward() #backpropagation\n",
    "                optimizer.step()  #update parameters\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()        \n",
    "        \n",
    "        epoch_loss = running_loss / data_lengths[phase]\n",
    "        print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "        if phase == 'train': train_loss.append(epoch_loss)\n",
    "        if phase == 'val': val_loss.append(epoch_loss)\n",
    "    print()\n",
    "          \n",
    "    if phase == 'val': #early stopping block        \n",
    "        if len(early_stop) == patience: del early_stop[0]        \n",
    "        early_stop.append(running_loss)\n",
    "        if min(early_stop) == early_stop[0] : break      \n",
    "\n",
    "print('\\nFinished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "loss_df1 = pd.read_csv('loss10-0.csv')\n",
    "loss_df2 = pd.read_csv('loss10-01.csv')\n",
    "loss_df_all = pd.concat([loss_df1,loss_df2],axis=0)\n",
    "train_loss = loss_df_all['Train Loss']\n",
    "val_loss = loss_df_all['Val Loss']\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "plt.plot(list(range(1,len(train_loss)+1)),train_loss,label='Train Loss')\n",
    "plt.plot(list(range(1,len(val_loss)+1)),val_loss,label='Val Loss')\n",
    "ax.set_title('Train/Val Loss Vs No. of Epochs')\n",
    "x_axis = plt.xlabel('No. of Epochs')\n",
    "y_axis = plt.ylabel('Train/Val Loss')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "int2char = pickle.load(open('int2char10.pickle', 'rb'))\n",
    "\n",
    "test = pd.read_csv('x_test_idx10.csv')\n",
    "x_test = test.astype(int)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed Text from Test data: \n",
      "theme song\n",
      "\n",
      "Greedy Sampling (max softmax probability): \n",
      "theme song story see film story see film story see film story see film story see film story see film story see\n",
      "\n",
      "Multinomial Sampling and Temperature scaling: \n",
      "theme song scent wind movie chear get see star film some actor dill show character also see make film love wor\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Temperature Scaling and multinormial sampling\n",
    "\n",
    "n = 1\n",
    "T = 0.3\n",
    "genlen = 100\n",
    "\n",
    "print('Seed Text from Test data: ')\n",
    "print(''.join([int2char[char] for char in  x_test.iloc[n,:].tolist()]))\n",
    "print()\n",
    "\n",
    "inputs = torch.stack([torch.cuda.FloatTensor(np.eye(37)[i]) for i in x_test.iloc[n,:].tolist()])\n",
    "inputs = inputs.reshape(1,10,37)\n",
    "\n",
    "generate = inputs.clone()\n",
    "with torch.no_grad():\n",
    "    for i in range (genlen):\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 2)\n",
    "        generate = torch.cat([generate,torch.cuda.FloatTensor(np.eye(37)[predicted][:,9]).reshape(1,1,37)],1)\n",
    "        inputs = torch.cat([inputs[:,1:,:],torch.cuda.FloatTensor(np.eye(37)[predicted][:,9]).reshape(1,1,37)],1)\n",
    "    \n",
    "gentext = [int(np.where(char==1)[0]) for char in generate[0]]\n",
    "\n",
    "print('Greedy Sampling (max softmax probability): ')\n",
    "print(''.join([int2char[char] for char in gentext]))\n",
    "print()\n",
    "\n",
    "inputs = torch.stack([torch.cuda.FloatTensor(np.eye(37)[i]) for i in x_test.iloc[n,:].tolist()])\n",
    "inputs = inputs.reshape(1,10,37)\n",
    "\n",
    "generate = inputs.clone()\n",
    "with torch.no_grad():\n",
    "    for i in range (genlen):\n",
    "        outputs= net(inputs)\n",
    "        nxt = torch.distributions.multinomial.Multinomial(total_count=1, probs= F.softmax(outputs.data[0,9,:]/T,0)).sample()\n",
    "        generate = torch.cat([generate,torch.cuda.FloatTensor(nxt).reshape(1,1,37)],1)\n",
    "        inputs = torch.cat([inputs[:,1:,:],torch.cuda.FloatTensor(nxt).reshape(1,1,37)],1)\n",
    "    \n",
    "gentext = [int(np.where(char==1)[0]) for char in generate[0]]\n",
    "\n",
    "print('Multinomial Sampling and Temperature scaling: ')\n",
    "print(''.join([int2char[char] for char in gentext]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#003366\"><b>Data was prepared in the same way as part3 except that window size was set to 10 here due to vanishing gradients in RNN compared to LSTM models. Also, as the implementation of RNN forward propagation from scratch invovles a loop, the run time is much longer than using pytorch's implementation. Hence, a simple single layer rnn with 512 recurrent units was used here. After 50 epochs, the model is able to generate a limited variety of words with correct spelling under multinomial sampling and low temeprature parameter while still having many words with spelling mistakes. The performance was clearly worse than part 3 mostly due to a lack of long range memory and also a simpler model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do the results of your basic RNN compare to your model in Chapter 3?**  What do you think explains the difference in performance? Discuss below.\n",
    "\n",
    "Some relevant resources on LSTMs (and RNN theory) below if you are interested:\n",
    "* http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "* https://www.youtube.com/watch?v=93rzMHtYT_0&list=LLpNVCNE9cYqVrjb2O8bZUGg&index=2&t=0s\n",
    "* https://www.youtube.com/watch?v=zQxm3Upr3_I\n",
    "* http://harinisuresh.com/2016/10/09/lstms/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Challenges (not required!):\n",
    "1. Build the forward pass of an LSTM, without using any existing RNN APIs (as above, with PyTorch or Tensorflow)\n",
    "1. Build a basic RNN or LSTM in Numpy - including forward pass as well as backpropogation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
